# -*- coding: utf-8 -*-
"""ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13OW2sO31TfNfaabD86uPbcwtXqKuTWti
"""

# =============================================================================
# C·∫§U H√åNH HU·∫§N LUY·ªÜN
# =============================================================================

NUM_EPOCHS = 30
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-4
BATCH_SIZE=16
EARLY_STOPPING_PATIENCE = 5  # D·ª´ng n·∫øu kh√¥ng c·∫£i thi·ªán sau N epochs
RESUME_TRAINING = True  # Resume t·ª´ checkpoint n·∫øu c√≥

# =============================================================================
# C·∫§U H√åNH M√îI TR∆Ø·ªúNG
# =============================================================================
print("="*70)
print("C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN CHO GOOGLE COLAB")
print("="*70)

print("üì¶ ƒêang c√†i ƒë·∫∑t PyTorch v·ªõi CUDA 11.8...")
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

print("üì¶ ƒêang c√†i ƒë·∫∑t transformers v√† timm...")
!pip install -q transformers timm

print("üì¶ ƒêang c√†i ƒë·∫∑t OpenCV, PIL, v√† c√°c th∆∞ vi·ªán x·ª≠ l√Ω ·∫£nh...")
!pip install -q opencv-python pillow numpy pandas matplotlib seaborn scikit-learn scikit-image

print("üì¶ ƒêang c√†i ƒë·∫∑t tqdm cho progress bar...")
!pip install -q tqdm

print("‚úì Ho√†n th√†nh c√†i ƒë·∫∑t th∆∞ vi·ªán!\n")

# Commented out IPython magic to ensure Python compatibility.
# =============================================================================
# IMPORT TH∆Ø VI·ªÜN C∆† B·∫¢N
# =============================================================================

import os
import json
import time
import warnings
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import Counter
from datetime import datetime

warnings.filterwarnings('ignore')

# =============================================================================
# IMPORT TH∆Ø VI·ªÜN X·ª¨ L√ù ·∫¢NH
# =============================================================================

import cv2
import numpy as np
import pandas as pd
from PIL import Image

# =============================================================================
# IMPORT TH∆Ø VI·ªÜN VISUALIZATION
# =============================================================================

import matplotlib.pyplot as plt
import seaborn as sns

# C·∫•u h√¨nh matplotlib
# %matplotlib inline
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3


# =============================================================================
# IMPORT TH∆Ø VI·ªÜN DEEP LEARNING
# =============================================================================

# =============================================================================
# IMPORT TH∆Ø VI·ªÜN DEEP LEARNING
# =============================================================================

# X·ª≠ l√Ω l·ªói import torch (AttributeError: partially initialized module)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader, random_split
    from torchvision import transforms
    import torchvision.transforms.functional as TF
except AttributeError as e:
    if "partially initialized module 'torch'" in str(e) or "'torch' has no attribute" in str(e):
        print("="*70)
        print("‚ö†Ô∏è  L·ªñI: CIRCULAR IMPORT HO·∫∂C CORRUPT TORCH MODULE")
        print("="*70)
        print("ƒêang th·ª≠ fix...")

        # X√≥a cache
        import sys
        modules_to_remove = [k for k in sys.modules.keys() if 'torch' in k]
        for module in modules_to_remove:
            if 'torch' in module:
                del sys.modules[module]

        # Reinstall PyTorch
        print("üì¶ ƒêang reinstall PyTorch...")
        get_ipython().system('pip uninstall -y torch torchvision torchaudio')
        get_ipython().system('pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir')

        # Th·ª≠ import l·∫°i
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import Dataset, DataLoader, random_split
        from torchvision import transforms
        import torchvision.transforms.functional as TF
        print("‚úì ƒê√£ fix v√† import torch th√†nh c√¥ng!")
    else:
        raise

# Ki·ªÉm tra GPU sau khi import torch
print("="*70)
print("KI·ªÇM TRA GPU")
print("="*70)
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"GPU Memory: {gpu_memory:.2f} GB")
else:
    print("‚ö†Ô∏è  CUDA kh√¥ng kh·∫£ d·ª•ng! H√£y ƒë·∫£m b·∫£o Runtime -> Change runtime type -> GPU ƒë∆∞·ª£c ch·ªçn")
print("="*70 + "\n")

# =============================================================================
# IMPORT VISION TRANSFORMER
# =============================================================================

# Import transformers
try:
    from transformers import ViTModel, ViTConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  transformers ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang c√†i...")
    !pip install -q transformers
    from transformers import ViTModel, ViTConfig
    TRANSFORMERS_AVAILABLE = True

# Import timm
try:
    import timm
    TIMM_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  timm ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang c√†i...")
    !pip install -q timm
    import timm
    TIMM_AVAILABLE = True

# =============================================================================
# IMPORT METRICS V√Ä UTILITIES
# =============================================================================

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score
)
from scipy import ndimage

# Progress bar
try:
    from tqdm import tqdm
    from tqdm.notebook import tqdm as tqdm_notebook
    USE_TQDM = True
except ImportError:
    USE_TQDM = False
    print("‚ö†Ô∏è  tqdm ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")

# =============================================================================
# HI·ªÇN TH·ªä TH√îNG TIN M√îI TR∆Ø·ªúNG
# =============================================================================

print("="*70)
print("TH√îNG TIN M√îI TR∆Ø·ªúNG")
print("="*70)
print(f"‚úì Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
print(f"‚úì PyTorch: {torch.__version__}")
print(f"‚úì Transformers: {TRANSFORMERS_AVAILABLE}")
print(f"‚úì TIMM: {TIMM_AVAILABLE}")
print(f"‚úì TQDM: {USE_TQDM}")
print("="*70)
print("‚úì ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán th√†nh c√¥ng!\n")

"""1.1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu"""

# =============================================================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU
# =============================================================================

print("="*70)
print("C·∫§U H√åNH GOOGLE DRIVE")
print("="*70)

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

print("\nüìÅ Ch·ªçn m·ªôt trong c√°c ph∆∞∆°ng √°n sau:")
print("   1. D·ªØ li·ªáu tr√™n Google Drive: /content/drive/MyDrive/ViT/datasets-sam-vit")
print("   2. Upload tr·ª±c ti·∫øp l√™n Colab: /content/ViT/datasets-sam-vit")

# T·ª± ƒë·ªông ph√°t hi·ªán ƒë∆∞·ªùng d·∫´n
drive_path = Path("/content/drive/MyDrive/ViT/datasets-sam-vit")

if drive_path.exists() and (drive_path / "images").exists():
    BASE_DATA_DIR = drive_path
    print(f"‚úì T√¨m th·∫•y d·ªØ li·ªáu tr√™n Google Drive: {BASE_DATA_DIR}")
else:
    print("‚ö†Ô∏è  Ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu!")

# C√°c th∆∞ m·ª•c con
IMAGES_DIR = BASE_DATA_DIR / "images"
MASKS_DIR = BASE_DATA_DIR / "masks"

# Mapping class IDs (t·ª´ mask values sang t√™n class)
CLASS_MAPPING = {
    0: "background",
    1: "brown_planthopper",      # R·∫ßy n√¢u (BPH) - Category ID 1
    2: "whitebacked_planthopper", # R·∫ßy l∆∞ng tr·∫Øng (WBPH) - Category ID 2
    3: "rice_leaf_miner"         # S√¢u ƒÉn l√° l√∫a (RLM) - Category ID 3
}

NUM_CLASSES = len([k for k in CLASS_MAPPING.keys() if k > 0])  # B·ªè background: 3 classes

print(f"\n{'='*70}")
print("KI·ªÇM TRA ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU")
print(f"{'='*70}")
print(f"BASE_DATA_DIR: {BASE_DATA_DIR}")
print(f"IMAGES_DIR: {IMAGES_DIR} ({'‚úì' if IMAGES_DIR.exists() else '‚úó'})")
print(f"MASKS_DIR: {MASKS_DIR} ({'‚úì' if MASKS_DIR.exists() else '‚úó'})")
print(f"\nS·ªë classes: {NUM_CLASSES}")
print(f"Class mapping:")
for k, v in CLASS_MAPPING.items():
    if k > 0:
        print(f"  {k}: {v}")

if not IMAGES_DIR.exists() or not MASKS_DIR.exists():
    print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Th∆∞ m·ª•c d·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i!")
    print("   Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n ho·∫∑c upload d·ªØ li·ªáu.")

"""## 1.2. ƒê·ªçc v√† ph√¢n t√≠ch d·ªØ li·ªáu (images v√† masks)"""

def analyze_dataset(images_dir: Path, masks_dir: Path, splits: List[str] = ['train', 'val', 'test']):
    """
    Ph√¢n t√≠ch dataset: ƒë·∫øm s·ªë l∆∞·ª£ng ·∫£nh, masks, classes theo t·ª´ng split
    ƒê·∫øm annotations (objects) v√† s·ªë h√¨nh ·∫£nh c√≥ t·ª´ng class
    """
    results = {}

    for split in splits:
        split_images_dir = images_dir / split
        split_masks_dir = masks_dir / split

        if not split_images_dir.exists() or not split_masks_dir.exists():
            print(f"‚ö†Ô∏è  Th∆∞ m·ª•c {split} kh√¥ng t·ªìn t·∫°i, b·ªè qua...")
            continue

        # T√¨m t·∫•t c·∫£ file ·∫£nh
        image_files = list(split_images_dir.glob("*.jpg")) + list(split_images_dir.glob("*.JPG")) + \
                     list(split_images_dir.glob("*.png")) + list(split_images_dir.glob("*.PNG"))

        # ƒê·∫øm masks v√† classes (lo·∫°i b·ªè file _vis.png)
        mask_files = [f for f in split_masks_dir.glob("*.png") if not f.stem.endswith("_vis")]

        # ƒê·∫øm classes trong masks
        class_annotation_counts = Counter()  # ƒê·∫øm s·ªë annotations (objects)
        class_image_counts = Counter()  # ƒê·∫øm s·ªë h√¨nh ·∫£nh c√≥ t·ª´ng class
        image_sizes = []
        valid_pairs = 0

        for img_file in image_files:
            mask_file = split_masks_dir / f"{img_file.stem}.png"
            if mask_file.exists():
                valid_pairs += 1
                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
                if mask is not None:
                    unique_classes = np.unique(mask)
                    for cls in unique_classes:
                        if cls > 0:  # B·ªè background
                            # ƒê·∫øm s·ªë annotations (connected components) cho class n√†y
                            class_mask = (mask == cls).astype(np.uint8)
                            num_labels, _ = cv2.connectedComponents(class_mask)
                            num_objects = num_labels - 1  # Tr·ª´ background label
                            if num_objects > 0:
                                class_annotation_counts[cls] += num_objects
                                class_image_counts[cls] += 1  # H√¨nh ·∫£nh n√†y c√≥ class n√†y

                    # ƒê·ªçc k√≠ch th∆∞·ªõc ·∫£nh
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w = img.shape[:2]
                        image_sizes.append((w, h))

        results[split] = {
            'total_images': len(image_files),
            'total_masks': len(mask_files),
            'valid_pairs': valid_pairs,
            'class_annotation_counts': dict(class_annotation_counts),
            'class_image_counts': dict(class_image_counts),
            'image_sizes': image_sizes
        }

        print(f"\n{'='*70}")
        print(f"Split: {split.upper()}")
        print(f"{'='*70}")
        print(f"T·ªïng s·ªë ·∫£nh: {results[split]['total_images']}")
        print(f"T·ªïng s·ªë masks: {results[split]['total_masks']}")
        print(f"S·ªë c·∫∑p ·∫£nh-mask h·ª£p l·ªá: {results[split]['valid_pairs']}")
        
        # T√≠nh t·ªïng annotations
        total_annotations = sum(results[split]['class_annotation_counts'].values())
        
        print(f"\nPh√¢n b·ªë classes:")
        print(f"{'='*70}")
        for cls_id in sorted(results[split]['class_annotation_counts'].keys()):
            cls_name = CLASS_MAPPING.get(cls_id, f"Unknown_{cls_id}")
            annotation_count = results[split]['class_annotation_counts'].get(cls_id, 0)
            image_count = results[split]['class_image_counts'].get(cls_id, 0)
            
            annotation_percentage = (annotation_count / total_annotations * 100) if total_annotations > 0 else 0
            
            print(f"  {cls_name} (ID={cls_id}):")
            print(f"    - S·ªë annotations: {annotation_count:,} ({annotation_percentage:.2f}%)")
            print(f"    - S·ªë h√¨nh ·∫£nh c√≥ lo·∫°i n√†y: {image_count:,}")

        if image_sizes:
            sizes_array = np.array(image_sizes)
            print(f"\nK√≠ch th∆∞·ªõc ·∫£nh (W x H):")
            print(f"  - Trung b√¨nh: {sizes_array[:, 0].mean():.0f} x {sizes_array[:, 1].mean():.0f}")
            print(f"  - Min: {sizes_array[:, 0].min()} x {sizes_array[:, 1].min()}")
            print(f"  - Max: {sizes_array[:, 0].max()} x {sizes_array[:, 1].max()}")

    return results

# Ph√¢n t√≠ch dataset
dataset_stats = analyze_dataset(IMAGES_DIR, MASKS_DIR)

"""## 1.3. Visualize m·ªôt s·ªë m·∫´u d·ªØ li·ªáu

"""

def visualize_samples(images_dir: Path, masks_dir: Path, split: str = 'train', samples_per_class: int = 2):
    """
    Hi·ªÉn th·ªã m·∫´u ·∫£nh v√† mask t∆∞∆°ng ·ª©ng, m·ªói class hi·ªÉn th·ªã 2 ·∫£nh
    """
    split_images_dir = images_dir / split
    split_masks_dir = masks_dir / split

    if not split_images_dir.exists() or not split_masks_dir.exists():
        print(f"‚ö†Ô∏è  Th∆∞ m·ª•c {split} kh√¥ng t·ªìn t·∫°i")
        return

    # T√¨m t·∫•t c·∫£ file ·∫£nh c√≥ mask t∆∞∆°ng ·ª©ng
    image_files = list(split_images_dir.glob("*.jpg")) + list(split_images_dir.glob("*.JPG")) + \
                  list(split_images_dir.glob("*.png")) + list(split_images_dir.glob("*.PNG"))
    
    # Ph√¢n lo·∫°i ·∫£nh theo class (t√¨m ·∫£nh c√≥ ch·ª©a t·ª´ng class)
    class_samples = {1: [], 2: [], 3: []}  # class_id -> list of (img_file, mask_file)
    
    for img_file in image_files:
        mask_file = split_masks_dir / f"{img_file.stem}.png"
        if not mask_file.exists():
            continue
            
        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue
            
        unique_classes = [c for c in np.unique(mask) if c > 0]
        
        # Th√™m v√†o danh s√°ch c·ªßa t·ª´ng class n·∫øu ch∆∞a ƒë·ªß
        for cls_id in unique_classes:
            if cls_id in class_samples and len(class_samples[cls_id]) < samples_per_class:
                class_samples[cls_id].append((img_file, mask_file))
                if len(class_samples[cls_id]) >= samples_per_class:
                    break

    # T·∫°o danh s√°ch samples theo th·ª© t·ª±: class 1 (2 ·∫£nh), class 2 (2 ·∫£nh), class 3 (2 ·∫£nh)
    valid_pairs = []
    class_labels = []  # L∆∞u nh√£n class cho m·ªói ·∫£nh
    
    for cls_id in sorted(class_samples.keys()):
        cls_name = CLASS_MAPPING.get(cls_id, f"Class_{cls_id}")
        for img_file, mask_file in class_samples[cls_id]:
            valid_pairs.append((img_file, mask_file))
            class_labels.append(f"{cls_name} (ID={cls_id})")

    if len(valid_pairs) == 0:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·∫∑p ·∫£nh-mask h·ª£p l·ªá trong split {split}")
        return

    num_samples = len(valid_pairs)
    
    # T·∫°o figure: 3 c·ªôt (·∫£nh g·ªëc, mask, overlay)
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))
    if num_samples == 1:
        axes = axes.reshape(1, -1)

    # Color map cho mask visualization (RGB)
    mask_color_map = {
        0: (0, 0, 0),       # Background - ƒëen
        1: (255, 0, 0),     # Brown Planthopper - ƒë·ªè
        2: (0, 255, 0),     # White-Backed Planthopper - xanh l√°
        3: (0, 0, 255),     # Rice Leaf Miner - xanh d∆∞∆°ng
    }

    for idx, ((img_file, mask_file), class_label) in enumerate(zip(valid_pairs, class_labels)):
        # ƒê·ªçc ·∫£nh
        img = cv2.imread(str(img_file))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # ƒê·ªçc mask
        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)

        # T·∫°o mask visualization m√†u
        mask_vis = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
        for cls_id, color in mask_color_map.items():
            mask_vis[mask == cls_id] = color

        # Overlay mask l√™n ·∫£nh
        overlay = img_rgb.copy()
        mask_colored = mask_vis.copy()
        mask_colored[mask == 0] = 0  # Ch·ªâ overlay c√°c pixel kh√¥ng ph·∫£i background
        overlay = cv2.addWeighted(overlay, 0.7, mask_colored, 0.3, 0)

        # Hi·ªÉn th·ªã c√°c classes c√≥ trong mask
        unique_classes = [c for c in np.unique(mask) if c > 0]
        classes_text = ", ".join([f"{CLASS_MAPPING[c]}(ID={c})" for c in unique_classes])

        # Hi·ªÉn th·ªã ·∫£nh g·ªëc
        axes[idx, 0].imshow(img_rgb)
        axes[idx, 0].set_title(f"·∫¢nh g·ªëc\n{class_label}\n{img_file.name}", fontsize=10, fontweight='bold')
        axes[idx, 0].axis('off')

        # Hi·ªÉn th·ªã mask
        axes[idx, 1].imshow(mask_vis)
        axes[idx, 1].set_title(f"Mask\n{class_label}\nClasses: {classes_text}", fontsize=10, fontweight='bold')
        axes[idx, 1].axis('off')

        # Hi·ªÉn th·ªã overlay
        axes[idx, 2].imshow(overlay)
        axes[idx, 2].set_title(f"Overlay\n{class_label}\nClasses: {classes_text}", fontsize=10, fontweight='bold')
        axes[idx, 2].axis('off')

    plt.tight_layout()
    plt.show()

# Visualize samples - m·ªói class hi·ªÉn th·ªã 2 ·∫£nh
visualize_samples(IMAGES_DIR, MASKS_DIR, split='train', samples_per_class=2)

"""## 1.4. Dataset class ƒë·ªÉ load d·ªØ li·ªáu cho ViT

Dataset n√†y s·∫Ω:
- ƒê·ªçc ·∫£nh g·ªëc v√† mask t∆∞∆°ng ·ª©ng
- Tr√≠ch xu·∫•t c√°c v√πng ROI t·ª´ mask (SAM ƒë√£ t·∫°o mask ch√≠nh x√°c)
- Crop v√† resize c√°c ROI v·ªÅ k√≠ch th∆∞·ªõc chu·∫©n cho ViT
- √Åp d·ª•ng augmentation n·∫øu c·∫ßn
"""

class PlanthopperDataset(Dataset):
    """
    Dataset class cho ph√¢n lo·∫°i r·∫ßy n√¢u, r·∫ßy l∆∞ng tr·∫Øng v√† s√¢u ƒÉn l√° l√∫a
    ƒê·ªçc ·∫£nh ƒë√£ ƒë∆∞·ª£c mask (t·ª´ SAM2) v√† tr√≠ch xu·∫•t ROI cho ViT
    """
    def __init__(
        self,
        images_dir: Path,
        masks_dir: Path,
        split: str,
        image_size: int = 224,
        augment: bool = False,
        extract_roi: bool = True,
        min_roi_size: int = 32,
    ):
        self.images_dir = images_dir / split
        self.masks_dir = masks_dir / split
        self.split = split
        self.image_size = image_size
        self.augment = augment and (split == 'train')
        self.extract_roi = extract_roi
        self.min_roi_size = min_roi_size

        # T√¨m t·∫•t c·∫£ c√°c c·∫∑p ·∫£nh-mask h·ª£p l·ªá
        image_files = list(self.images_dir.glob("*.jpg")) + list(self.images_dir.glob("*.png")) + \
                     list(self.images_dir.glob("*.JPG")) + list(self.images_dir.glob("*.PNG"))

        self.samples = []
        for img_file in image_files:
            mask_file = self.masks_dir / f"{img_file.stem}.png"
            if mask_file.exists():
                # Ki·ªÉm tra mask c√≥ d·ªØ li·ªáu kh√¥ng
                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
                if mask is not None and np.sum(mask > 0) > 0:
                    if extract_roi:
                        # Tr√≠ch xu·∫•t t·ª´ng ROI ri√™ng bi·ªát
                        rois = self._extract_rois(img_file, mask_file, mask)
                        self.samples.extend(rois)
                    else:
                        # S·ª≠ d·ª•ng to√†n b·ªô ·∫£nh
                        self.samples.append({
                            'image_file': img_file,
                            'mask_file': mask_file,
                            'roi_bbox': None,  # To√†n b·ªô ·∫£nh
                            'label': None  # S·∫Ω x√°c ƒë·ªãnh t·ª´ mask
                        })

        print(f"‚úì Split {split}: {len(self.samples)} samples")

        # Augmentation transforms
        if self.augment:
            self.aug_transform = transforms.Compose([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            ])
        else:
            self.aug_transform = None

        # Normalization (ImageNet mean/std)
        self.normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )

    def _extract_rois(self, image_file: Path, mask_file: Path, mask: np.ndarray) -> List[Dict]:
        """
        Tr√≠ch xu·∫•t c√°c ROI (Regions of Interest) t·ª´ mask
        M·ªói ROI t∆∞∆°ng ·ª©ng v·ªõi m·ªôt instance c·ªßa m·ªôt class
        """
        rois = []
        unique_classes = [c for c in np.unique(mask) if c > 0]  # B·ªè background

        for class_id in unique_classes:
            # T·∫°o binary mask cho class n√†y
            class_mask = (mask == class_id).astype(np.uint8)

            # T√¨m contours ƒë·ªÉ t√°ch c√°c instance ri√™ng bi·ªát
            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for contour in contours:
                area = cv2.contourArea(contour)
                if area < self.min_roi_size * self.min_roi_size:  # B·ªè qua ROI qu√° nh·ªè
                    continue

                # L·∫•y bounding box
                x, y, w, h = cv2.boundingRect(contour)

                # M·ªü r·ªông bbox m·ªôt ch√∫t ƒë·ªÉ c√≥ context
                padding = 10
                x = max(0, x - padding)
                y = max(0, y - padding)
                w = min(mask.shape[1] - x, w + 2 * padding)
                h = min(mask.shape[0] - y, h + 2 * padding)

                rois.append({
                    'image_file': image_file,
                    'mask_file': mask_file,
                    'roi_bbox': (x, y, w, h),
                    'label': class_id - 1  # Chuy·ªÉn t·ª´ [1,2,3] sang [0,1,2] cho ViT
                })

        return rois

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # ƒê·ªçc ·∫£nh
        img = cv2.imread(str(sample['image_file']))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if sample['roi_bbox'] is not None:
            # Crop ROI
            x, y, w, h = sample['roi_bbox']
            roi = img_rgb[y:y+h, x:x+w]
        else:
            # S·ª≠ d·ª•ng to√†n b·ªô ·∫£nh
            roi = img_rgb

        # Resize v·ªÅ image_size
        roi_pil = Image.fromarray(roi)
        roi_resized = roi_pil.resize((self.image_size, self.image_size), Image.BILINEAR)

        # Convert to tensor
        img_tensor = TF.to_tensor(roi_resized)

        # Augmentation
        if self.aug_transform is not None:
            img_tensor = self.aug_transform(img_tensor)

        # Normalize
        img_tensor = self.normalize(img_tensor)

        # Label
        label = sample['label']
        if label is None:
            # X√°c ƒë·ªãnh label t·ª´ mask n·∫øu ch∆∞a c√≥
            mask = cv2.imread(str(sample['mask_file']), cv2.IMREAD_GRAYSCALE)
            unique_classes = [c for c in np.unique(mask) if c > 0]
            label = unique_classes[0] - 1 if unique_classes else 0

        return img_tensor, torch.tensor(label, dtype=torch.long)

# Test dataset
print("T·∫°o datasets...")
train_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'train', augment=True)
val_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'val', augment=False)
test_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'test', augment=False)

print(f"\nTrain: {len(train_dataset)} samples")
print(f"Val: {len(val_dataset)} samples")
print(f"Test: {len(test_dataset)} samples")

# =============================================================================
# LOAD SAM2 MODEL (Optional - ch·ªâ c·∫ßn n·∫øu mu·ªën t·∫°o masks m·ªõi)
# =============================================================================

LOAD_SAM = False  # ƒê·∫∑t True n·∫øu c·∫ßn s·ª≠ d·ª•ng SAM

if LOAD_SAM:
    from sam2.build_sam import build_sam2
    from sam2.sam2_image_predictor import SAM2ImagePredictor

    # ƒê∆∞·ªùng d·∫´n checkpoint v√† config
    SAM_CHECKPOINT = "./checkpoints/sam2.1_hiera_tiny.pt"
    SAM_MODEL_CFG = "configs/sam2.1/sam2.1_hiera_t.yaml"

    if os.path.exists(SAM_CHECKPOINT):
        print("Loading SAM2 model...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam_model = build_sam2(SAM_MODEL_CFG, SAM_CHECKPOINT, device=device)
        sam_predictor = SAM2ImagePredictor(sam_model)
        print(f"‚úì SAM2 loaded on {device}")
    else:
        print(f"‚ö†Ô∏è  SAM checkpoint not found at {SAM_CHECKPOINT}")
        print("   Skip loading SAM (masks ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn)")
        sam_predictor = None
else:
    print("Skip loading SAM (masks ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn t·ª´ b∆∞·ªõc preprocessing)")
    sam_predictor = None

"""## 2.2. X√¢y d·ª±ng Vision Transformer (ViT) Model

S·ª≠ d·ª•ng ViT ƒë·ªÉ ph√¢n lo·∫°i c√°c ROI ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ SAM masks.
"""

class ViTClassifier(nn.Module):
    """
    Vision Transformer classifier cho ph√¢n lo·∫°i r·∫ßy
    S·ª≠ d·ª•ng pretrained ViT v√† th√™m classification head
    ∆Øu: D·ªÖ truy c·∫≠p c·∫•u h√¨nh, ecosystem HuggingFace.
    Nh∆∞·ª£c: Ch·∫≠m h∆°n ƒë√¥i ch√∫t, √≠t bi·∫øn th·ªÉ model h∆°n.
    """
    def __init__(
        self,
        num_classes: int = 3,
        model_name: str = 'google/vit-base-patch16-224',
        pretrained: bool = True,
        dropout: float = 0.1,
    ):
        super().__init__()

        # Load pretrained ViT
        if pretrained:
            self.vit = ViTModel.from_pretrained(model_name)
        else:
            config = ViTConfig.from_pretrained(model_name)
            self.vit = ViTModel(config)

        # Feature dimension
        hidden_size = self.vit.config.hidden_size

        # Classification head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )

        # Freeze m·ªôt ph·∫ßn encoder n·∫øu mu·ªën (optional)
        # for param in list(self.vit.encoder.layer[:6].parameters()):
        #     param.requires_grad = False

    def forward(self, x):
        # ViT forward
        outputs = self.vit(pixel_values=x)

        # L·∫•y [CLS] token
        cls_token = outputs.last_hidden_state[:, 0]

        # Classification
        logits = self.classifier(cls_token)

        return logits

# Ho·∫∑c s·ª≠ d·ª•ng timm (alternative)
class ViTClassifierTimm(nn.Module):
    """
    ViT classifier s·ª≠ d·ª•ng timm library
    ∆Øu: Nhanh, nhi·ªÅu bi·∫øn th·ªÉ (vit_base_patch16_224, vit_base_patch16_384, vit_base_patch8_224, ‚Ä¶).
    Khuy·∫øn ngh·ªã cho Colab/T4.
    """
    def __init__(
        self,
        num_classes: int = 3,
        model_name: str = 'vit_base_patch16_224',
        pretrained: bool = True,
        dropout: float = 0.1,
    ):
        super().__init__()

        # Load pretrained ViT t·ª´ timm
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=0,  # Ch·ªâ l·∫•y features
        )

        feature_dim = self.backbone.num_features

        # Classification head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(feature_dim, feature_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feature_dim // 2, num_classes)
        )

    def forward(self, x):
        features = self.backbone(x)
        logits = self.classifier(features)
        return logits

# T·∫°o model
print("T·∫°o ViT model...")
model = ViTClassifierTimm(
    num_classes=NUM_CLASSES,
    model_name='vit_base_patch16_224',  # C√≥ th·ªÉ thay b·∫±ng 'vit_small_patch16_224' cho model nh·ªè h∆°n
    pretrained=True,
    dropout=0.1
)

# Test forward pass
dummy_input = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output = model(dummy_input)
print(f"‚úì Model created!")
print(f"  Input shape: {dummy_input.shape}")
print(f"  Output shape: {output.shape}")
print(f"  Number of parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

# =============================================================================
# T·∫†O DATALOADERS
# =============================================================================

NUM_WORKERS = 0

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=True if torch.cuda.is_available() else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True if torch.cuda.is_available() else False
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"‚úì DataLoaders created!")
print(f"  Train batches: {len(train_loader)}")
print(f"  Val batches: {len(val_loader)}")
print(f"  Test batches: {len(test_loader)}")

# Ki·ªÉm tra m·ªôt batch
sample_batch = next(iter(train_loader))
images, labels = sample_batch
print(f"\nSample batch:")
print(f"  Images shape: {images.shape}")
print(f"  Labels shape: {labels.shape}")
print(f"  Labels: {labels[:10].tolist()}...")  # Hi·ªÉn th·ªã 10 labels ƒë·∫ßu

"""## 3.2. H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√°

"""

def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """Hu·∫•n luy·ªán m·ªôt epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    # Progress bar
    if USE_TQDM:
        pbar = tqdm_notebook(train_loader, desc=f'Epoch {epoch+1} [Train]', leave=False)
    else:
        pbar = train_loader

    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

        # Forward
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward
        loss.backward()

        # Gradient clipping ƒë·ªÉ tr√°nh gradient explosion
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Update progress bar
        if USE_TQDM:
            current_acc = 100. * correct / total
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{current_acc:.2f}%'
            })
        elif (batch_idx + 1) % 50 == 0:
            print(f'  Batch [{batch_idx+1}/{len(train_loader)}], '
                  f'Loss: {loss.item():.4f}, '
                  f'Acc: {100.*correct/total:.2f}%')

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

def evaluate(model, data_loader, criterion, device, class_names=None, desc='Eval'):
    """ƒê√°nh gi√° model tr√™n validation/test set"""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    # Progress bar
    if USE_TQDM:
        pbar = tqdm_notebook(data_loader, desc=desc, leave=False)
    else:
        pbar = data_loader

    with torch.no_grad():
        for images, labels in pbar:
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            if USE_TQDM:
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})

    epoch_loss = running_loss / len(data_loader)
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    # T√≠nh c√°c metrics
    accuracy = 100. * np.mean(all_preds == all_labels)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)

    # Per-class metrics
    per_class_metrics = {}
    if class_names is None:
        class_names = [f"Class_{i}" for i in range(len(np.unique(all_labels)))]

    unique_labels = np.unique(all_labels)
    for cls_id in unique_labels:
        cls_mask = all_labels == cls_id
        if np.sum(cls_mask) > 0:
            cls_acc = 100. * np.mean(all_preds[cls_mask] == all_labels[cls_mask])
            cls_precision = precision_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)
            cls_recall = recall_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)
            cls_f1 = f1_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)

            per_class_metrics[class_names[cls_id]] = {
                'accuracy': cls_acc,
                'precision': cls_precision,
                'recall': cls_recall,
                'f1': cls_f1
            }

    return {
        'loss': epoch_loss,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': all_preds,
        'labels': all_labels,
        'per_class': per_class_metrics
    }

print("‚úì Training v√† evaluation functions ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"{'='*70}")
print("C·∫§U H√åNH HU·∫§N LUY·ªÜN")
print(f"{'='*70}")
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Epochs: {NUM_EPOCHS}")
print(f"Learning rate: {LEARNING_RATE}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Early stopping patience: {EARLY_STOPPING_PATIENCE}")
print(f"{'='*70}")

# Di chuy·ªÉn model l√™n device
model = model.to(device)

# Loss function (CrossEntropy v·ªõi class weights ƒë·ªÉ x·ª≠ l√Ω class imbalance)
# T√≠nh class weights t·ª´ dataset
print("\nüìä T√≠nh to√°n class weights...")
class_counts = np.zeros(NUM_CLASSES)
print("   ƒêang ƒë·∫øm samples trong train dataset...")
for _, label in train_dataset:
    class_counts[label] += 1

total_samples = np.sum(class_counts)
class_weights = total_samples / (NUM_CLASSES * class_counts)
class_weights = torch.FloatTensor(class_weights).to(device)

print(f"   Class counts: {class_counts}")
print(f"   Class weights: {class_weights.cpu().numpy()}")
print(f"   Total samples: {total_samples}")

criterion = nn.CrossEntropyLoss(weight=class_weights)

# Optimizer
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# Learning rate scheduler (CosineAnnealingLR v·ªõi warmup)
warmup_epochs = 3
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=NUM_EPOCHS - warmup_epochs,
    eta_min=1e-6
)

# L∆∞u training history
history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': [],
    'val_f1': [],
    'learning_rates': []
}

# Best model
best_val_f1 = 0.0
best_epoch = 0
best_model_state = None
early_stopping_counter = 0

# Resume training n·∫øu c·∫ßn
start_epoch = 0
if RESUME_TRAINING:
    checkpoint_path = Path("/content/drive/MyDrive/ViT_checkpoints/vit_classifier_best.pth")
    if checkpoint_path.exists():
        print(f"\nüìÇ Resume training t·ª´ checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint.get('epoch', 0)
        best_val_f1 = checkpoint.get('best_val_f1', 0.0)
        history = checkpoint.get('history', history)
        print(f"   Resume t·ª´ epoch {start_epoch}, best F1: {best_val_f1:.4f}")

class_names = [CLASS_MAPPING[i+1] for i in range(NUM_CLASSES)]

print(f"\n{'='*70}")
print("B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN")
print(f"{'='*70}")
print(f"Start epoch: {start_epoch}")
print(f"Class names: {class_names}")
print(f"{'='*70}\n")

# =============================================================================
# TRAINING LOOP
# =============================================================================

training_start_time = time.time()

for epoch in range(start_epoch, NUM_EPOCHS):
    epoch_start_time = time.time()

    print(f"\n{'='*70}")
    print(f"Epoch [{epoch+1}/{NUM_EPOCHS}]")
    print(f"{'='*70}")

    # Warmup learning rate cho 3 epochs ƒë·∫ßu
    if epoch < warmup_epochs:
        warmup_lr = LEARNING_RATE * (epoch + 1) / warmup_epochs
        for param_group in optimizer.param_groups:
            param_group['lr'] = warmup_lr
    else:
        scheduler.step()

    current_lr = optimizer.param_groups[0]['lr']
    history['learning_rates'].append(current_lr)

    # Train
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)

    # Validate
    val_results = evaluate(model, val_loader, criterion, device, class_names=class_names, desc=f'Epoch {epoch+1} [Val]')

    # Save history
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_results['loss'])
    history['val_acc'].append(val_results['accuracy'])
    history['val_f1'].append(val_results['f1'])

    # Print results
    epoch_time = time.time() - epoch_start_time
    print(f"\nTrain - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
    print(f"Val   - Loss: {val_results['loss']:.4f}, "
          f"Acc: {val_results['accuracy']:.2f}%, "
          f"F1: {val_results['f1']:.4f}")
    print(f"      - Precision: {val_results['precision']:.4f}, "
          f"Recall: {val_results['recall']:.4f}")
    print(f"      - LR: {current_lr:.6f}, Time: {epoch_time:.1f}s")

    # Print per-class metrics
    print("\n      Per-class metrics:")
    for cls_name, metrics in val_results['per_class'].items():
        print(f"        {cls_name}: "
              f"Acc={metrics['accuracy']:.2f}%, "
              f"P={metrics['precision']:.4f}, "
              f"R={metrics['recall']:.4f}, "
              f"F1={metrics['f1']:.4f}")

    # Save best model
    improved = False
    if val_results['f1'] > best_val_f1:
        best_val_f1 = val_results['f1']
        best_epoch = epoch + 1
        best_model_state = model.state_dict().copy()
        early_stopping_counter = 0
        improved = True
        print(f"\n      ‚úÖ New best F1: {best_val_f1:.4f} (Epoch {best_epoch})")

        # L∆∞u checkpoint tr√™n Colab
        checkpoint_dir = Path("/content/drive/MyDrive/ViT_checkpoints")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        checkpoint = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'epoch': epoch + 1,
            'best_val_f1': best_val_f1,
            'best_epoch': best_epoch,
            'test_results': val_results,
            'class_names': class_names,
            'history': history,
            'config': {
                'num_classes': NUM_CLASSES,
                'learning_rate': LEARNING_RATE,
                'batch_size': BATCH_SIZE,
                'class_mapping': CLASS_MAPPING
            }
        }

        checkpoint_path = checkpoint_dir / 'vit_classifier_best.pth'
        torch.save(checkpoint, checkpoint_path)
        print(f"      üíæ Saved checkpoint: {checkpoint_path}")
    else:
        early_stopping_counter += 1
        print(f"\n      ‚è≥ No improvement ({early_stopping_counter}/{EARLY_STOPPING_PATIENCE})")

    # Early stopping
    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:
        print(f"\n{'='*70}")
        print(f"‚èπÔ∏è  EARLY STOPPING")
        print(f"{'='*70}")
        print(f"Kh√¥ng c·∫£i thi·ªán sau {EARLY_STOPPING_PATIENCE} epochs")
        print(f"Best F1: {best_val_f1:.4f} t·∫°i epoch {best_epoch}")
        break

# Load best model
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print(f"\n‚úì Loaded best model (F1: {best_val_f1:.4f} t·∫°i epoch {best_epoch})")

total_training_time = time.time() - training_start_time
print(f"\n{'='*70}")
print("HU·∫§N LUY·ªÜN HO√ÄN T·∫§T")
print(f"{'='*70}")
print(f"Total training time: {total_training_time/60:.1f} minutes")
print(f"Best validation F1: {best_val_f1:.4f}")
print(f"Best epoch: {best_epoch}")
print(f"{'='*70}")

"""## 3.4. Visualize training history

"""

# V·∫Ω ƒë·ªì th·ªã training history
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Loss
axes[0].plot(history['train_loss'], label='Train Loss', marker='o')
axes[0].plot(history['val_loss'], label='Val Loss', marker='s')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training and Validation Loss')
axes[0].legend()
axes[0].grid(True)

# Accuracy
axes[1].plot(history['train_acc'], label='Train Acc', marker='o')
axes[1].plot(history['val_acc'], label='Val Acc', marker='s')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy (%)')
axes[1].set_title('Training and Validation Accuracy')
axes[1].legend()
axes[1].grid(True)

# F1 Score
axes[2].plot(history['val_f1'], label='Val F1', marker='s', color='green')
axes[2].set_xlabel('Epoch')
axes[2].set_ylabel('F1 Score')
axes[2].set_title('Validation F1 Score')
axes[2].legend()
axes[2].grid(True)

plt.tight_layout()
plt.show()

"""## 3.5. ƒê√°nh gi√° tr√™n test set

"""

# ƒê√°nh gi√° tr√™n test set
print("="*70)
print("ƒê√ÅNH GI√Å TR√äN TEST SET")
print("="*70)

test_results = evaluate(model, test_loader, criterion, device, class_names=class_names, desc='Test')

print(f"\nTest Results:")
print(f"  Loss: {test_results['loss']:.4f}")
print(f"  Accuracy: {test_results['accuracy']:.2f}%")
print(f"  Precision: {test_results['precision']:.4f}")
print(f"  Recall: {test_results['recall']:.4f}")
print(f"  F1 Score: {test_results['f1']:.4f}")

print(f"\nPer-class metrics:")
for cls_name, metrics in test_results['per_class'].items():
    print(f"  {cls_name}:")
    print(f"    Accuracy: {metrics['accuracy']:.2f}%")
    print(f"    Precision: {metrics['precision']:.4f}")
    print(f"    Recall: {metrics['recall']:.4f}")
    print(f"    F1: {metrics['f1']:.4f}")

# Classification report
print(f"\nClassification Report:")
print(classification_report(
    test_results['labels'],
    test_results['predictions'],
    target_names=class_names
))

# Confusion matrix
cm = confusion_matrix(test_results['labels'], test_results['predictions'])
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix - Test Set (SAM-ViT)', fontsize=14, fontweight='bold')
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.show()

"""# 4. PH√ÇN T√çCH K·∫æT QU·∫¢, X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò V√Ä SO S√ÅNH

## 4.1. T√≠nh IoU v√† Dice Coefficient cho Segmentation Masks

T√≠nh to√°n IoU (Intersection over Union) v√† Dice coefficient ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng segmentation masks t·ª´ SAM.
"""

# Visualize m·ªôt s·ªë m·∫´u d·ª± ƒëo√°n
def visualize_predictions(model, test_loader, device, num_samples=12):
    """Hi·ªÉn th·ªã m·ªôt s·ªë m·∫´u d·ª± ƒëo√°n tr√™n test set"""
    model.eval()

    # L·∫•y m·ªôt batch t·ª´ test_loader
    dataiter = iter(test_loader)
    images, labels = next(dataiter)
    images = images.to(device)

    with torch.no_grad():
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)

    images = images.cpu()
    labels = labels.cpu().numpy()
    predicted = predicted.cpu().numpy()
    probabilities = probabilities.cpu().numpy()

    # T·∫°o figure
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    axes = axes.flatten()

    class_names = [CLASS_MAPPING[i+1] for i in range(NUM_CLASSES)]

    for idx in range(min(num_samples, len(images))):
        img = images[idx]
        true_label = labels[idx]
        pred_label = predicted[idx]
        prob = probabilities[idx]

        # Denormalize image
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        img_denorm = img * std + mean
        img_denorm = torch.clamp(img_denorm, 0, 1)
        img_np = img_denorm.permute(1, 2, 0).numpy()

        # Hi·ªÉn th·ªã
        axes[idx].imshow(img_np)

        # Title v·ªõi th√¥ng tin prediction
        true_name = class_names[true_label]
        pred_name = class_names[pred_label]
        correct = "‚úì" if true_label == pred_label else "‚úó"

        title = f"{correct} True: {true_name}\nPred: {pred_name} ({prob[pred_label]:.2f})"
        color = 'green' if true_label == pred_label else 'red'

        axes[idx].set_title(title, color=color, fontsize=9, fontweight='bold')
        axes[idx].axis('off')

    plt.suptitle('Visualization: Predictions tr√™n Test Set (SAM-ViT)',
                 fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.show()

# Visualize predictions
print("\nVisualizing predictions...")
visualize_predictions(model, test_loader, device, num_samples=12)

# =============================================================================
# T√çNH IOU V√Ä DICE COEFFICIENT CHO SEGMENTATION MASKS
# =============================================================================

def calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:
    """
    T√≠nh IoU (Intersection over Union) gi·ªØa hai masks
    """
    intersection = np.logical_and(mask1, mask2).sum()
    union = np.logical_or(mask1, mask2).sum()
    if union == 0:
        return 1.0 if intersection == 0 else 0.0
    return intersection / union

def calculate_dice_coefficient(mask1: np.ndarray, mask2: np.ndarray) -> float:
    """
    T√≠nh Dice coefficient gi·ªØa hai masks
    Dice = 2 * |A ‚à© B| / (|A| + |B|)
    """
    intersection = np.logical_and(mask1, mask2).sum()
    mask1_area = mask1.sum()
    mask2_area = mask2.sum()
    if mask1_area + mask2_area == 0:
        return 1.0 if intersection == 0 else 0.0
    return 2.0 * intersection / (mask1_area + mask2_area)

def evaluate_segmentation_masks(masks_dir: Path, split: str = 'test', class_ids: List[int] = None):
    """
    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng segmentation masks t·ª´ SAM

    Note: Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta ch·ªâ c√≥ masks t·ª´ SAM (kh√¥ng c√≥ ground truth masks ƒë·ªÉ so s√°nh),
    n√™n s·∫Ω ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng masks d·ª±a tr√™n:
    - S·ªë l∆∞·ª£ng pixels ƒë∆∞·ª£c segment cho m·ªói class
    - ƒê·ªô t√°ch bi·ªát gi·ªØa c√°c instances
    - Ph√¢n b·ªë m·∫≠t ƒë·ªô
    """
    split_masks_dir = masks_dir / split

    if not split_masks_dir.exists():
        print(f"‚ö†Ô∏è  Th∆∞ m·ª•c {split_masks_dir} kh√¥ng t·ªìn t·∫°i")
        return None

    mask_files = list(split_masks_dir.glob("*.png"))
    # B·ªè qua c√°c file _vis.png
    mask_files = [f for f in mask_files if not f.name.endswith('_vis.png')]

    if class_ids is None:
        class_ids = [1, 2, 3]  # Brown, White-backed, Rice leaf miner

    results = {
        'total_masks': len(mask_files),
        'class_stats': {},
        'average_mask_quality': {}
    }

    all_iou_per_class = {cls_id: [] for cls_id in class_ids}
    all_dice_per_class = {cls_id: [] for cls_id in class_ids}

    print(f"ƒêang ƒë√°nh gi√° {len(mask_files)} masks t·ª´ split {split}...")

    for mask_file in tqdm_notebook(mask_files, desc='Evaluating masks', leave=False) if USE_TQDM else mask_files:
        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue

        # T√≠nh to√°n cho t·ª´ng class
        for class_id in class_ids:
            class_mask = (mask == class_id).astype(np.uint8)

            if class_mask.sum() == 0:
                continue

            # T√°ch c√°c instances ri√™ng bi·ªát
            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if len(contours) > 1:
                # T√≠nh IoU v√† Dice gi·ªØa c√°c instances (ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t√°ch bi·ªát)
                # T·∫°o binary mask cho m·ªói instance v√† so s√°nh v·ªõi to√†n b·ªô class mask
                for i, contour1 in enumerate(contours[:5]):  # Gi·ªõi h·∫°n 5 instances ƒë·∫ßu
                    instance_mask1 = np.zeros_like(class_mask)
                    cv2.fillPoly(instance_mask1, [contour1], 1)

                    # So s√°nh instance v·ªõi to√†n b·ªô class mask
                    iou = calculate_iou(instance_mask1, class_mask)
                    dice = calculate_dice_coefficient(instance_mask1, class_mask)

                    all_iou_per_class[class_id].append(iou)
                    all_dice_per_class[class_id].append(dice)

    # T√≠nh th·ªëng k√™
    for class_id in class_ids:
        cls_name = CLASS_MAPPING.get(class_id, f"Class_{class_id}")
        if len(all_iou_per_class[class_id]) > 0:
            results['class_stats'][cls_name] = {
                'mean_iou': np.mean(all_iou_per_class[class_id]),
                'mean_dice': np.mean(all_dice_per_class[class_id]),
                'num_instances': len(all_iou_per_class[class_id])
            }

    return results

# ƒê√°nh gi√° masks
print("="*70)
print("ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG SEGMENTATION MASKS")
print("="*70)

segmentation_results = {}
for split in ['train', 'val', 'test']:
    print(f"\nƒêang ƒë√°nh gi√° split: {split}")
    results = evaluate_segmentation_masks(MASKS_DIR, split=split)
    if results:
        segmentation_results[split] = results

# Hi·ªÉn th·ªã k·∫øt qu·∫£
print(f"\n{'='*70}")
print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å SEGMENTATION MASKS")
print(f"{'='*70}")

for split, results in segmentation_results.items():
    print(f"\nSplit: {split.upper()}")
    print(f"  T·ªïng s·ªë masks: {results['total_masks']}")
    print(f"  Th·ªëng k√™ theo class:")
    for cls_name, stats in results['class_stats'].items():
        print(f"    {cls_name}:")
        print(f"      Mean IoU: {stats['mean_iou']:.4f}")
        print(f"      Mean Dice: {stats['mean_dice']:.4f}")
        print(f"      S·ªë instances: {stats['num_instances']}")

"""## 4.2. X√°c ƒë·ªãnh m·∫≠t ƒë·ªô r·∫ßy n√¢u t·ª´ Masks

T√≠nh to√°n m·∫≠t ƒë·ªô r·∫ßy n√¢u (s·ªë l∆∞·ª£ng c√° th·ªÉ/ƒë∆°n v·ªã di·ªán t√≠ch) t·ª´ segmentation masks ƒë·ªÉ h·ªó tr·ª£ ƒë√°nh gi√° m·ª©c ƒë·ªô x√¢m nhi·ªÖm.

"""

# =============================================================================
# X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò R·∫¶Y N√ÇU T·ª™ MASKS
# =============================================================================

def calculate_planthopper_density(masks_dir: Path, images_dir: Path, split: str = 'test'):
    """
    T√≠nh m·∫≠t ƒë·ªô r·∫ßy n√¢u t·ª´ segmentation masks

    M·∫≠t ƒë·ªô ƒë∆∞·ª£c t√≠nh b·∫±ng:
    - S·ªë l∆∞·ª£ng instances (c√° th·ªÉ) / ·∫£nh
    - S·ªë l∆∞·ª£ng pixels / di·ªán t√≠ch ·∫£nh (pixels/cm¬≤ n·∫øu c√≥ th√¥ng tin scale)
    """
    split_masks_dir = masks_dir / split
    split_images_dir = images_dir / split

    if not split_masks_dir.exists() or not split_images_dir.exists():
        print(f"‚ö†Ô∏è  Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i")
        return None

    mask_files = list(split_masks_dir.glob("*.png"))
    mask_files = [f for f in mask_files if not f.name.endswith('_vis.png')]

    density_stats = {
        'total_images': 0,
        'images_with_planthoppers': 0,
        'total_instances': 0,
        'instances_per_image': [],
        'pixels_per_image': [],
        'class_density': {1: [], 2: [], 3: []},  # Brown, White-backed, Rice leaf miner
        'per_image_stats': []
    }

    print(f"ƒêang t√≠nh m·∫≠t ƒë·ªô r·∫ßy t·ª´ {len(mask_files)} masks...")

    for mask_file in tqdm_notebook(mask_files, desc='Calculating density', leave=False) if USE_TQDM else mask_files:
        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue

        # ƒê·ªçc ·∫£nh ƒë·ªÉ l·∫•y k√≠ch th∆∞·ªõc
        img_file = split_images_dir / f"{mask_file.stem}.jpg"
        if not img_file.exists():
            img_file = split_images_dir / f"{mask_file.stem}.png"

        img_height, img_width = mask.shape
        image_area_pixels = img_height * img_width

        # ƒê·∫øm instances cho t·ª´ng class
        total_instances = 0
        image_stats = {
            'image_name': mask_file.stem,
            'image_size': (img_width, img_height),
            'instances': {},
            'pixel_counts': {},
            'density': {}
        }

        for class_id in [1, 2, 3]:
            cls_name = CLASS_MAPPING.get(class_id, f"Class_{class_id}")
            class_mask = (mask == class_id).astype(np.uint8)
            pixel_count = class_mask.sum()

            if pixel_count == 0:
                image_stats['instances'][cls_name] = 0
                image_stats['pixel_counts'][cls_name] = 0
                image_stats['density'][cls_name] = 0.0
                continue

            # ƒê·∫øm s·ªë instances (contours)
            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            num_instances = len(contours)
            total_instances += num_instances

            # M·∫≠t ƒë·ªô (instances / di·ªán t√≠ch ·∫£nh * 10000 ƒë·ªÉ chu·∫©n h√≥a)
            density = (num_instances / image_area_pixels) * 10000 if image_area_pixels > 0 else 0

            image_stats['instances'][cls_name] = num_instances
            image_stats['pixel_counts'][cls_name] = pixel_count
            image_stats['density'][cls_name] = density

            density_stats['class_density'][class_id].append({
                'instances': num_instances,
                'pixels': pixel_count,
                'density': density
            })

        density_stats['total_images'] += 1
        if total_instances > 0:
            density_stats['images_with_planthoppers'] += 1
            density_stats['total_instances'] += total_instances
            density_stats['instances_per_image'].append(total_instances)
            density_stats['pixels_per_image'].append(mask[mask > 0].sum())
            density_stats['per_image_stats'].append(image_stats)

    # T√≠nh th·ªëng k√™ t·ªïng h·ª£p
    if len(density_stats['instances_per_image']) > 0:
        density_stats['mean_instances_per_image'] = np.mean(density_stats['instances_per_image'])
        density_stats['std_instances_per_image'] = np.std(density_stats['instances_per_image'])
        density_stats['max_instances_per_image'] = np.max(density_stats['instances_per_image'])
        density_stats['min_instances_per_image'] = np.min(density_stats['instances_per_image'])

        # Th·ªëng k√™ theo class
        for class_id in [1, 2, 3]:
            cls_name = CLASS_MAPPING.get(class_id, f"Class_{class_id}")
            if len(density_stats['class_density'][class_id]) > 0:
                instances_list = [d['instances'] for d in density_stats['class_density'][class_id]]
                density_stats[f'{cls_name}_mean_instances'] = np.mean(instances_list)
                density_stats[f'{cls_name}_mean_density'] = np.mean([d['density'] for d in density_stats['class_density'][class_id]])

    return density_stats

# T√≠nh m·∫≠t ƒë·ªô r·∫ßy
print("="*70)
print("X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò R·∫¶Y N√ÇU T·ª™ MASKS")
print("="*70)

density_results = {}
for split in ['train', 'val', 'test']:
    print(f"\nƒêang t√≠nh m·∫≠t ƒë·ªô cho split: {split}")
    results = calculate_planthopper_density(MASKS_DIR, IMAGES_DIR, split=split)
    if results:
        density_results[split] = results

# Hi·ªÉn th·ªã k·∫øt qu·∫£
print(f"\n{'='*70}")
print("K·∫æT QU·∫¢ M·∫¨T ƒê·ªò R·∫¶Y N√ÇU")
print(f"{'='*70}")

for split, results in density_results.items():
    print(f"\nSplit: {split.upper()}")
    print(f"  T·ªïng s·ªë ·∫£nh: {results['total_images']}")
    print(f"  S·ªë ·∫£nh c√≥ r·∫ßy: {results['images_with_planthoppers']}")
    print(f"  T·ªïng s·ªë c√° th·ªÉ (instances): {results['total_instances']}")

    if 'mean_instances_per_image' in results:
        print(f"\n  Th·ªëng k√™ m·∫≠t ƒë·ªô:")
        print(f"    Trung b√¨nh: {results['mean_instances_per_image']:.2f} c√° th·ªÉ/·∫£nh")
        print(f"    ƒê·ªô l·ªách chu·∫©n: {results['std_instances_per_image']:.2f}")
        print(f"    T·ªëi ƒëa: {results['max_instances_per_image']} c√° th·ªÉ/·∫£nh")
        print(f"    T·ªëi thi·ªÉu: {results['min_instances_per_image']} c√° th·ªÉ/·∫£nh")

        print(f"\n  M·∫≠t ƒë·ªô theo lo√†i:")
        for class_id in [1, 2, 3]:
            cls_name = CLASS_MAPPING.get(class_id, f"Class_{class_id}")
            mean_inst_key = f'{cls_name}_mean_instances'
            mean_dens_key = f'{cls_name}_mean_density'
            if mean_inst_key in results:
                print(f"    {cls_name}:")
                print(f"      Trung b√¨nh: {results[mean_inst_key]:.2f} c√° th·ªÉ/·∫£nh")
                print(f"      M·∫≠t ƒë·ªô chu·∫©n h√≥a: {results[mean_dens_key]:.4f}")

# Visualize ph√¢n b·ªë m·∫≠t ƒë·ªô
if density_results:
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Ph√¢n b·ªë s·ªë instances/·∫£nh
    all_instances = []
    for split, results in density_results.items():
        all_instances.extend(results.get('instances_per_image', []))

    if all_instances:
        axes[0].hist(all_instances, bins=30, edgecolor='black', alpha=0.7)
        axes[0].set_xlabel('S·ªë l∆∞·ª£ng c√° th·ªÉ r·∫ßy/·∫£nh', fontsize=11)
        axes[0].set_ylabel('S·ªë l∆∞·ª£ng ·∫£nh', fontsize=11)
        axes[0].set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng c√° th·ªÉ r·∫ßy tr√™n m·ªói ·∫£nh', fontsize=12, fontweight='bold')
        axes[0].axvline(np.mean(all_instances), color='red', linestyle='--',
                       label=f'Trung b√¨nh: {np.mean(all_instances):.2f}')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

    # So s√°nh m·∫≠t ƒë·ªô theo lo√†i
    class_names = ['Brown\nPlanthopper', 'White-backed\nPlanthopper', 'Rice Leaf\nMiner']
    mean_instances = []
    for class_id in [1, 2, 3]:
        cls_name = CLASS_MAPPING[class_id]
        all_instances_class = []
        for split, results in density_results.items():
            for item in results['class_density'][class_id]:
                all_instances_class.append(item['instances'])
        mean_instances.append(np.mean(all_instances_class) if all_instances_class else 0)

    axes[1].bar(class_names, mean_instances, color=['#ff6b6b', '#4ecdc4', '#45b7d1'], alpha=0.7)
    axes[1].set_ylabel('S·ªë l∆∞·ª£ng c√° th·ªÉ trung b√¨nh/·∫£nh', fontsize=11)
    axes[1].set_title('So s√°nh m·∫≠t ƒë·ªô theo lo√†i r·∫ßy', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.show()

"""## 4.3. So s√°nh v·ªõi m√¥ h√¨nh tham chi·∫øu SwinT-YOLOv8-p2

So s√°nh k·∫øt qu·∫£ c·ªßa SAM-ViT v·ªõi m√¥ h√¨nh tham chi·∫øu SwinT-YOLOv8-p2 tr√™n c√°c metrics: Precision, Recall, F1-score, mAP.
"""

# =============================================================================
# SO S√ÅNH V·ªöI SWINT-YOLOV8-P2
# =============================================================================

print("="*70)
print("SO S√ÅNH V·ªöI M√î H√åNH THAM CHI·∫æU: SWINT-YOLOV8-P2")
print("="*70)

# K·∫øt qu·∫£ t·ª´ SwinT-YOLOv8-p2 (t·ª´ ƒë·ªÅ t√†i)
swint_yolov8_results = {
    'mAP@0.5': 0.847,
    'F1_score': 0.899,
    'Precision': 0.857,  # Gi·∫£ ƒë·ªãnh t·ª´ mAP v√† F1
    'Recall': 0.943,     # Gi·∫£ ƒë·ªãnh t·ª´ mAP v√† F1
    'FPS': 16.69,
    'description': 'SwinT-YOLOv8-p2: Detection model v·ªõi Swin Transformer v√† SCConv'
}

# K·∫øt qu·∫£ t·ª´ SAM-ViT
sam_vit_results = {
    'Accuracy': test_results['accuracy'] / 100,  # Chuy·ªÉn t·ª´ % sang decimal
    'F1_score': test_results['f1'],
    'Precision': test_results['precision'],
    'Recall': test_results['recall'],
    'description': 'SAM-ViT: Segmentation (SAM) + Classification (ViT)'
}

# So s√°nh
comparison_df = pd.DataFrame({
    'Metric': ['mAP@0.5 / Accuracy', 'Precision', 'Recall', 'F1-Score', 'FPS'],
    'SwinT-YOLOv8-p2': [
        swint_yolov8_results['mAP@0.5'],
        swint_yolov8_results['Precision'],
        swint_yolov8_results['Recall'],
        swint_yolov8_results['F1_score'],
        swint_yolov8_results['FPS']
    ],
    'SAM-ViT': [
        sam_vit_results['Accuracy'],
        sam_vit_results['Precision'],
        sam_vit_results['Recall'],
        sam_vit_results['F1_score'],
        'N/A (Classification task)'
    ]
})

print("\nB·∫£ng so s√°nh k·∫øt qu·∫£:")
print("="*70)
print(comparison_df.to_string(index=False))
print("="*70)

# T√≠nh ph·∫ßn trƒÉm c·∫£i thi·ªán
improvement = {}
for metric in ['Precision', 'Recall', 'F1_score']:
    sam_vit_val = sam_vit_results[metric]
    swint_val = swint_yolov8_results[metric]
    if isinstance(sam_vit_val, (int, float)) and isinstance(swint_val, (int, float)):
        improvement[metric] = ((sam_vit_val - swint_val) / swint_val) * 100

print("\nPh·∫ßn trƒÉm c·∫£i thi·ªán c·ªßa SAM-ViT so v·ªõi SwinT-YOLOv8-p2:")
for metric, pct in improvement.items():
    sign = "+" if pct >= 0 else ""
    print(f"  {metric}: {sign}{pct:.2f}%")

# Visualize comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar chart so s√°nh metrics
metrics_compare = ['Precision', 'Recall', 'F1-Score']
swint_vals = [swint_yolov8_results[m] for m in ['Precision', 'Recall', 'F1_score']]
sam_vit_vals = [sam_vit_results[m] for m in ['Precision', 'Recall', 'F1_score']]

x = np.arange(len(metrics_compare))
width = 0.35

axes[0].bar(x - width/2, swint_vals, width, label='SwinT-YOLOv8-p2', alpha=0.8, color='#3498db')
axes[0].bar(x + width/2, sam_vit_vals, width, label='SAM-ViT', alpha=0.8, color='#e74c3c')
axes[0].set_ylabel('Score', fontsize=11)
axes[0].set_title('So s√°nh Precision, Recall, F1-Score', fontsize=12, fontweight='bold')
axes[0].set_xticks(x)
axes[0].set_xticklabels(metrics_compare)
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].set_ylim([0, 1])

# Accuracy/Detection rate comparison
models = ['SwinT-YOLOv8-p2\n(mAP@0.5)', 'SAM-ViT\n(Accuracy)']
acc_values = [swint_yolov8_results['mAP@0.5'], sam_vit_results['Accuracy']]
colors = ['#3498db', '#e74c3c']

axes[1].bar(models, acc_values, alpha=0.8, color=colors)
axes[1].set_ylabel('Score', fontsize=11)
axes[1].set_title('So s√°nh mAP@0.5 (Detection) vs Accuracy (Classification)',
                  fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].set_ylim([0, 1])

# Th√™m gi√° tr·ªã tr√™n bars
for i, (model, val) in enumerate(zip(models, acc_values)):
    axes[1].text(i, val + 0.02, f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("NH·∫¨N X√âT SO S√ÅNH")
print("="*70)
print("""
üìä SwinT-YOLOv8-p2:
   - M√¥ h√¨nh Detection (YOLO) v·ªõi Swin Transformer
   - T·ªëc ƒë·ªô inference cao (16.69 FPS)
   - Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng d·ª±a tr√™n bounding boxes
   - mAP@0.5 = 0.847, F1 = 0.899

üìä SAM-ViT (ƒê·ªÅ xu·∫•t):
   - M√¥ h√¨nh k·∫øt h·ª£p Segmentation (SAM) + Classification (ViT)
   - SAM t·∫°o masks ch√≠nh x√°c ·ªü c·∫•p ƒë·ªô pixel
   - ViT ph√¢n lo·∫°i t·ª´ng ROI ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ masks
   - ∆Øu ƒëi·ªÉm: Ph√¢n ƒëo·∫°n chi ti·∫øt, x√°c ƒë·ªãnh m·∫≠t ƒë·ªô ch√≠nh x√°c
   - Nh∆∞·ª£c ƒëi·ªÉm: Ch·∫≠m h∆°n (2-stage pipeline)

üéØ K·∫øt lu·∫≠n:
   - SAM-ViT ph√π h·ª£p cho b√†i to√°n c·∫ßn ph√¢n ƒëo·∫°n chi ti·∫øt v√† x√°c ƒë·ªãnh m·∫≠t ƒë·ªô
   - SwinT-YOLOv8-p2 ph√π h·ª£p cho b√†i to√°n real-time detection
   - Hai ph∆∞∆°ng ph√°p b·ªï sung cho nhau: SwinT-YOLOv8-p2 ƒë·ªÉ detect nhanh,
     SAM-ViT ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt v√† ƒë√°nh gi√° m·∫≠t ƒë·ªô
""")

# L∆∞u k·∫øt qu·∫£ so s√°nh
comparison_csv = Path("/content/drive/MyDrive/ViT_checkpoints/comparison_with_swint_yolov8.csv")
comparison_df.to_csv(comparison_csv, index=False)
print(f"\n‚úì ƒê√£ l∆∞u b·∫£ng so s√°nh: {comparison_csv}")

print("\n" + "="*70)
print("T√ìM T·∫ÆT K·∫æT QU·∫¢ SAM-ViT")
print("="*70)

print(f"\nTest Set Performance:")
print(f"  Accuracy: {test_results['accuracy']:.2f}%")
print(f"  Precision: {test_results['precision']:.4f}")
print(f"  Recall: {test_results['recall']:.4f}")
print(f"  F1 Score: {test_results['f1']:.4f}")

if segmentation_results:
    print(f"\nSegmentation Quality:")
    for split, seg_results in segmentation_results.items():
        print(f"  {split.upper()}:")
        for cls_name, stats in seg_results.get('class_stats', {}).items():
            print(f"    {cls_name} - Mean IoU: {stats['mean_iou']:.4f}, Mean Dice: {stats['mean_dice']:.4f}")

if density_results:
    print(f"\nDensity Analysis:")
    for split, dens_results in density_results.items():
        if 'mean_instances_per_image' in dens_results:
            print(f"  {split.upper()}: {dens_results['mean_instances_per_image']:.2f} c√° th·ªÉ/·∫£nh")

print("\n" + "="*70)
print("ƒê·ªÄ XU·∫§T H∆Ø·ªöNG M·ªû R·ªòNG")
print("="*70)
print("""
1. T·ªëi ∆∞u hi·ªáu su·∫•t:
   - C·∫£i thi·ªán t·ªëc ƒë·ªô inference b·∫±ng c√°ch t·ªëi ∆∞u SAM pipeline
   - S·ª≠ d·ª•ng SAM nhanh h∆°n ho·∫∑c quantize model
   - K·∫øt h·ª£p SAM-ViT v·ªõi SwinT-YOLOv8-p2: d√πng YOLO ƒë·ªÉ detect nhanh,
     SAM-ViT ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt nh·ªØng v√πng c√≥ r·∫ßy

2. C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c:
   - Fine-tune SAM tr√™n d·ªØ li·ªáu r·∫ßy n√¢u c·ª• th·ªÉ
   - Ensemble nhi·ªÅu ViT models v·ªõi c√°c architecture kh√°c nhau
   - S·ª≠ d·ª•ng attention mechanisms ƒë·ªÉ t·∫≠p trung v√†o v√πng r·∫ßy nh·ªè

3. X·ª≠ l√Ω m·∫≠t ƒë·ªô cao:
   - X·ª≠ l√Ω tr∆∞·ªùng h·ª£p r·∫ßy ch·ªìng l·∫•p (overlapping instances)
   - C·∫£i thi·ªán kh·∫£ nƒÉng ph√¢n t√°ch c√°c c√° th·ªÉ g·∫ßn nhau
   - S·ª≠ d·ª•ng instance segmentation thay v√¨ semantic segmentation

4. ·ª®ng d·ª•ng th·ª±c t·∫ø:
   - T√≠ch h·ª£p v√†o h·ªá th·ªëng gi√°m s√°t t·ª± ƒë·ªông tr√™n ƒë·ªìng ru·ªông
   - Ph√°t tri·ªÉn ·ª©ng d·ª•ng mobile ƒë·ªÉ n√¥ng d√¢n s·ª≠ d·ª•ng
   - K·∫øt h·ª£p v·ªõi IoT sensors v√† drone ƒë·ªÉ gi√°m s√°t quy m√¥ l·ªõn
   - X√¢y d·ª±ng h·ªá th·ªëng d·ª± b√°o d·ªãch h·∫°i d·ª±a tr√™n m·∫≠t ƒë·ªô

5. ƒê√°nh gi√° m·ªü r·ªông:
   - Test tr√™n nhi·ªÅu ƒëi·ªÅu ki·ªán √°nh s√°ng, g√≥c ch·ª•p kh√°c nhau
   - ƒê√°nh gi√° hi·ªáu qu·∫£ tr√™n c√°c giai ƒëo·∫°n ph√°t tri·ªÉn kh√°c nhau c·ªßa l√∫a
   - Cross-validation ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh v√† kh·∫£ nƒÉng t·ªïng qu√°t h√≥a
""")

print("\n‚úì Ho√†n th√†nh ph√¢n t√≠ch v√† so s√°nh!")


# =============================================================================
# L∆ØU MODEL V√Ä K·∫æT QU·∫¢
# =============================================================================

print("\n" + "="*70)
print("L∆ØU MODEL V√Ä K·∫æT QU·∫¢")
print("="*70)

# Ki·ªÉm tra v√† kh√¥i ph·ª•c model n·∫øu c·∫ßn
if not isinstance(model, nn.Module):
    print("‚ö†Ô∏è  Model kh√¥ng ph·∫£i l√† ƒë·ªëi t∆∞·ª£ng PyTorch model, ƒëang kh√¥i ph·ª•c...")

    # Th·ª≠ load t·ª´ best checkpoint
    checkpoint_path = Path("/content/drive/MyDrive/ViT_checkpoints/vit_classifier_best.pth")
    if checkpoint_path.exists() and best_model_state is not None:
        print(f"   ƒêang t·∫°o l·∫°i model v√† load t·ª´ best_model_state...")
        # T·∫°o l·∫°i model
        model = ViTClassifierTimm(
            num_classes=NUM_CLASSES,
            model_name='vit_base_patch16_224',
            pretrained=False,  # Kh√¥ng c·∫ßn pretrained v√¨ s·∫Ω load weights
            dropout=0.1
        )
        model = model.to(device)
        model.load_state_dict(best_model_state)
        print("   ‚úì ƒê√£ kh√¥i ph·ª•c model t·ª´ best_model_state")
    elif checkpoint_path.exists():
        print(f"   ƒêang load t·ª´ checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        # T·∫°o l·∫°i model
        model = ViTClassifierTimm(
            num_classes=NUM_CLASSES,
            model_name='vit_base_patch16_224',
            pretrained=False,
            dropout=0.1
        )
        model = model.to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("   ‚úì ƒê√£ load model t·ª´ checkpoint")
    else:
        raise ValueError("Kh√¥ng th·ªÉ kh√¥i ph·ª•c model! Vui l√≤ng ch·∫°y l·∫°i training.")

# L∆∞u v√†o Google Drive
save_dir = Path("/content/drive/MyDrive/ViT_checkpoints")
save_dir.mkdir(parents=True, exist_ok=True)

# L∆∞u model cu·ªëi c√πng
final_checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict() if isinstance(optimizer, optim.Optimizer) else None,
    'epoch': NUM_EPOCHS,
    'best_val_f1': best_val_f1,
    'best_epoch': best_epoch,
    'test_results': test_results,
    'class_names': class_names,
    'history': history,
    'segmentation_results': segmentation_results,
    'density_results': density_results,
    'config': {
        'num_classes': NUM_CLASSES,
        'learning_rate': LEARNING_RATE,
        'batch_size': BATCH_SIZE,
        'class_mapping': CLASS_MAPPING
    }
}

final_path = save_dir / 'vit_classifier_final.pth'
torch.save(final_checkpoint, final_path)
print(f"‚úì Final model ƒë√£ ƒë∆∞·ª£c l∆∞u: {final_path}")

# Export predictions v√† results
predictions_df = pd.DataFrame({
    'true_label': [class_names[i] for i in test_results['labels']],
    'predicted_label': [class_names[i] for i in test_results['predictions']],
    'correct': test_results['labels'] == test_results['predictions']
})

results_csv = save_dir / 'test_results.csv'
predictions_df.to_csv(results_csv, index=False)
print(f"‚úì Test results ƒë√£ ƒë∆∞·ª£c l∆∞u: {results_csv}")

# L∆∞u summary v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
summary = {
    'model': 'SAM-ViT',
    'best_val_f1': best_val_f1,
    'best_epoch': best_epoch,
    'test_metrics': {
        'accuracy': test_results['accuracy'],
        'precision': test_results['precision'],
        'recall': test_results['recall'],
        'f1': test_results['f1']
    },
    'per_class_metrics': test_results['per_class'],
    'segmentation_quality': segmentation_results,
    'density_analysis': density_results,
    'comparison_with_swint_yolov8': {
        'swint_yolov8': swint_yolov8_results,
        'sam_vit': sam_vit_results,
        'improvement': improvement
    }
}

summary_json = save_dir / 'summary.json'
with open(summary_json, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)
print(f"‚úì Summary ƒë√£ ƒë∆∞·ª£c l∆∞u: {summary_json}")

# L∆∞u density results
if density_results:
    density_json = save_dir / 'density_results.json'
    with open(density_json, 'w', encoding='utf-8') as f:
        json.dump(density_results, f, indent=2, ensure_ascii=False, default=str)
    print(f"‚úì Density results ƒë√£ ƒë∆∞·ª£c l∆∞u: {density_json}")

print(f"\nüìÅ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {save_dir}")
# Export predictions summary
predictions_df = pd.DataFrame({
    'true_label': [class_names[i] for i in test_results['labels']],
    'predicted_label': [class_names[i] for i in test_results['predictions']],
    'correct': test_results['labels'] == test_results['predictions']
})

print("\n" + "="*70)
print("T√ìM T·∫ÆT PREDICTIONS")
print("="*70)
print(predictions_df.groupby('true_label')['correct'].agg(['count', 'sum', 'mean']))
print("="*70)