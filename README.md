# SAM 2: H∆∞·ªõng d·∫´n ch·∫°y project

**[AI at Meta, FAIR](https://ai.meta.com/research/)**

SAM 2 (Segment Anything Model 2) l√† m·ªôt m√¥ h√¨nh AI m·∫°nh m·∫Ω ƒë·ªÉ ph√¢n ƒëo·∫°n ƒë·ªëi t∆∞·ª£ng trong h√¨nh ·∫£nh v√† video. ƒê√¢y l√† h∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ thi·∫øt l·∫≠p v√† ch·∫°y project n√†y.

![SAM 2 architecture](assets/model_diagram.png?raw=true)

## üìã Y√™u c·∫ßu h·ªá th·ªëng

### Ph·∫ßn c·ª©ng:
- **GPU**: NVIDIA GPU v·ªõi CUDA support (khuy·∫øn ngh·ªã)
- **RAM**: T·ªëi thi·ªÉu 8GB, khuy·∫øn ngh·ªã 16GB+
- **·ªî c·ª©ng**: √çt nh·∫•t 10GB dung l∆∞·ª£ng tr·ªëng

### Ph·∫ßn m·ªÅm:
- **Python**: ‚â• 3.10
- **PyTorch**: ‚â• 2.5.1
- **CUDA**: Phi√™n b·∫£n t∆∞∆°ng th√≠ch v·ªõi PyTorch (th∆∞·ªùng l√† CUDA 12.1)
- **H·ªá ƒëi·ªÅu h√†nh**: Linux (khuy·∫øn ngh·ªã), Windows v·ªõi WSL

## üöÄ C√†i ƒë·∫∑t nhanh

### B∆∞·ªõc 1: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Python

```bash
# T·∫°o m√¥i tr∆∞·ªùng conda m·ªõi
conda create --name sam2 python=3.12 --yes

# K√≠ch ho·∫°t m√¥i tr∆∞·ªùng
conda activate sam2

# C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### B∆∞·ªõc 2: C√†i ƒë·∫∑t SAM2

```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c project
cd sam2

# C√†i ƒë·∫∑t SAM2 v·ªõi notebooks
pip install -e ".[notebooks]"
```

**N·∫øu g·∫∑p l·ªói CUDA extension:**
```bash
# B·ªè qua CUDA extension n·∫øu c·∫ßn thi·∫øt
SAM2_BUILD_CUDA=0 pip install -e ".[notebooks]"
```

### B∆∞·ªõc 3: T·∫£i xu·ªëng model checkpoints

```bash
# T·∫£i xu·ªëng t·∫•t c·∫£ checkpoints
cd checkpoints
./download_ckpts.sh
cd ..
```

**Ho·∫∑c t·∫£i xu·ªëng t·ª´ng model ri√™ng l·∫ª:**
- [sam2.1_hiera_tiny.pt](https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt) (38.9M) - Nhanh nh·∫•t
- [sam2.1_hiera_small.pt](https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt) (46M) - C√¢n b·∫±ng
- [sam2.1_hiera_base_plus.pt](https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt) (80.8M) - Khuy·∫øn ngh·ªã
- [sam2.1_hiera_large.pt](https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt) (224.4M) - Ch·∫•t l∆∞·ª£ng cao nh·∫•t

## üéØ C√°ch s·ª≠ d·ª•ng

### Ki·ªÉm tra c√†i ƒë·∫∑t

Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, h√£y ki·ªÉm tra xem SAM2 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng ch∆∞a:

```bash
# Ki·ªÉm tra Python v√† PyTorch
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"

# Ki·ªÉm tra SAM2
python -c "import sam2; print('SAM2 installed successfully!')"
```

### 1. Ph√¢n ƒëo·∫°n h√¨nh ·∫£nh

**Ch·∫°y notebook v√≠ d·ª•:**
```bash
jupyter notebook notebooks/image_predictor_example.ipynb
```

**Code Python c∆° b·∫£n:**
```python
import torch
import cv2
import numpy as np
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

# Thi·∫øt l·∫≠p model
checkpoint = "./checkpoints/sam2.1_hiera_base_plus.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

# Load h√¨nh ·∫£nh
image_path = "path/to/your/image.jpg"
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# S·ª≠ d·ª•ng model
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(image_rgb)
    
    # Click point (x, y) v√† label (1 = foreground, 0 = background)
    input_point = np.array([[x, y]])
    input_label = np.array([1])
    
    # Predict mask
    masks, scores, logits = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=True,
    )
    
    # L·∫•y mask t·ªët nh·∫•t
    best_mask = masks[np.argmax(scores)]
    print(f"Mask score: {scores[np.argmax(scores)]:.3f}")
```

**V√≠ d·ª• v·ªõi box prompt:**
```python
# S·ª≠ d·ª•ng box thay v√¨ point
input_box = np.array([x1, y1, x2, y2])  # [left, top, right, bottom]

masks, scores, logits = predictor.predict(
    point_coords=None,
    point_labels=None,
    box=input_box[None, :],
    multimask_output=False,
)
```

### 2. Ph√¢n ƒëo·∫°n video

```python
import torch
import cv2
from sam2.build_sam import build_sam2_video_predictor

# Thi·∫øt l·∫≠p video predictor
checkpoint = "./checkpoints/sam2.1_hiera_base_plus.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

# Load video
video_path = "path/to/your/video.mp4"

# Initialize state
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    state = predictor.init_state(video_path)
    
    # Th√™m prompt ban ƒë·∫ßu
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(
        state, 
        points=[[x, y]], 
        labels=[1]
    )
    
    # Propagate qua to√†n b·ªô video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        print(f"Frame {frame_idx}: {len(object_ids)} objects tracked")
        # X·ª≠ l√Ω masks cho t·ª´ng frame
        for obj_id, mask in zip(object_ids, masks):
            # Visualize ho·∫∑c l∆∞u mask
            pass
```

### 3. T·ª± ƒë·ªông t·∫°o mask

```bash
jupyter notebook notebooks/automatic_mask_generator_example.ipynb
```

**Code t·ª± ƒë·ªông t·∫°o mask:**
```python
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

# Thi·∫øt l·∫≠p automatic mask generator
mask_generator = SAM2AutomaticMaskGenerator(
    model=build_sam2(model_cfg, checkpoint),
    points_per_side=32,
    pred_iou_thresh=0.9,
    stability_score_thresh=0.95,
    crop_n_layers=1,
    crop_n_points_downscale_factor=2,
    min_mask_region_area=100,
)

# T·∫°o masks t·ª± ƒë·ªông
masks = mask_generator.generate(image_rgb)
print(f"Generated {len(masks)} masks")
```

## üñ•Ô∏è Ch·∫°y tr√™n m√°y c·ªßa b·∫°n

### Th√¥ng tin h·ªá th·ªëng hi·ªán t·∫°i
- **Python**: 3.12.0 (64-bit) ‚úÖ
- **Platform**: Windows ‚úÖ  
- **PyTorch**: Ch∆∞a c√†i ƒë·∫∑t ‚ùå
- **SAM2**: Ch∆∞a c√†i ƒë·∫∑t ‚ùå

### H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t t·ª´ng b∆∞·ªõc cho m√°y c·ªßa b·∫°n

**Ph∆∞∆°ng √°n 1: C√†i ƒë·∫∑t Anaconda (Khuy·∫øn ngh·ªã)**
```bash
# T·∫£i xu·ªëng t·ª´: https://www.anaconda.com/products/distribution
# C√†i ƒë·∫∑t v√† m·ªü Anaconda Prompt
# Sau ƒë√≥ ch·∫°y:
conda create --name sam2 python=3.10 --yes
conda activate sam2
```

**Ph∆∞∆°ng √°n 2: S·ª≠ d·ª•ng Python c√≥ s·∫µn (Cho m√°y kh√¥ng c√≥ Conda)**
```bash
# Ki·ªÉm tra Python hi·ªán t·∫°i
python --version

# N√¢ng c·∫•p pip
python -m pip install --upgrade pip

# T·∫°o virtual environment
python -m venv sam2_env

# K√≠ch ho·∫°t virtual environment
# Tr√™n Windows:
sam2_env\Scripts\activate
# Tr√™n Linux/Mac:
source sam2_env/bin/activate
```

**B∆∞·ªõc ti·∫øp theo: C√†i ƒë·∫∑t PyTorch**
```bash
# C√†i ƒë·∫∑t PyTorch v·ªõi CUDA (n·∫øu c√≥ GPU NVIDIA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ho·∫∑c c√†i ƒë·∫∑t CPU version (khuy·∫øn ngh·ªã cho m√°y c·ªßa b·∫°n)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

**Ki·ªÉm tra PyTorch:**
```bash
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"
```

**C√†i ƒë·∫∑t SAM2:**
```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c project
cd D:\3.Research\3.VisionTransformer\3.Project\sam2

# C√†i ƒë·∫∑t SAM2
pip install -e ".[notebooks]"
```

**N·∫øu g·∫∑p l·ªói CUDA extension:**
```bash
# B·ªè qua CUDA extension n·∫øu c·∫ßn thi·∫øt
SAM2_BUILD_CUDA=0 pip install -e ".[notebooks]"
```

**T·∫£i xu·ªëng model:**
```bash
# T·∫£i xu·ªëng model nh·ªè ƒë·ªÉ test
cd checkpoints
curl -O https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt
cd ..
```

**Test c√†i ƒë·∫∑t:**
```bash
# Ki·ªÉm tra PyTorch
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"

# Ki·ªÉm tra SAM2
python -c "import sam2; print('SAM2 installed successfully!')"
```

### Ch·∫°y v√≠ d·ª• ƒë·∫ßu ti√™n

**T·∫°o file test ƒë∆°n gi·∫£n:**
```bash
# T·∫°o file test_sam2.py
echo 'import torch
import sys
print("Python version:", sys.version)
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

try:
    import sam2
    print("‚úÖ SAM2 installed successfully!")
except ImportError as e:
    print("‚ùå SAM2 not installed:", e)
' > test_sam2.py
```

**Ch·∫°y test:**
```bash
python test_sam2.py
```

**N·∫øu SAM2 ƒë√£ c√†i ƒë·∫∑t th√†nh c√¥ng, t·∫°o file demo:**
```python
# T·∫°o file demo_sam2.py
import torch
import cv2
import numpy as np
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

# Thi·∫øt l·∫≠p model
checkpoint = "./checkpoints/sam2.1_hiera_tiny.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_t.yaml"

try:
    predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))
    print("‚úÖ SAM2 model loaded successfully!")
    print("B·∫°n c√≥ th·ªÉ ch·∫°y c√°c notebook v√≠ d·ª•:")
    print("- notebooks/image_predictor_example.ipynb")
    print("- notebooks/video_predictor_example.ipynb")
    print("- notebooks/automatic_mask_generator_example.ipynb")
except Exception as e:
    print("‚ùå Error loading model:", e)
```

**Ch·∫°y demo:**
```bash
python demo_sam2.py
```

### üö® L∆∞u √Ω quan tr·ªçng cho m√°y c·ªßa b·∫°n

**V√¨ m√°y c·ªßa b·∫°n kh√¥ng c√≥ Conda:**
1. **S·ª≠ d·ª•ng virtual environment** ƒë·ªÉ tr√°nh xung ƒë·ªôt packages
2. **C√†i ƒë·∫∑t CPU version** c·ªßa PyTorch (ph√π h·ª£p v·ªõi m√°y kh√¥ng c√≥ GPU)
3. **B·∫Øt ƒë·∫ßu v·ªõi model tiny** ƒë·ªÉ test nhanh
4. **Ch·∫°y tr√™n CPU** s·∫Ω ch·∫≠m nh∆∞ng ·ªïn ƒë·ªãnh

**Th·ªùi gian d·ª± ki·∫øn:**
- C√†i ƒë·∫∑t PyTorch: 5-10 ph√∫t
- C√†i ƒë·∫∑t SAM2: 10-15 ph√∫t  
- Ph√¢n ƒëo·∫°n 1 h√¨nh ·∫£nh: 30-60 gi√¢y

## üåê Ch·∫°y Web Demo

### C√°ch 1: S·ª≠ d·ª•ng Docker (Khuy·∫øn ngh·ªã)

```bash
# Ch·∫°y c·∫£ frontend v√† backend
docker compose up --build
```

**Truy c·∫≠p:**
- Frontend: http://localhost:7262
- Backend: http://localhost:7263/graphql

### C√°ch 2: Ch·∫°y local

**C√†i ƒë·∫∑t dependencies cho demo:**
```bash
pip install -e '.[interactive-demo]'
conda install -c conda-forge ffmpeg
```

**Ch·∫°y backend:**
```bash
cd demo/backend/server/

PYTORCH_ENABLE_MPS_FALLBACK=1 \
APP_ROOT="$(pwd)/../../../" \
API_URL=http://localhost:7263 \
MODEL_SIZE=base_plus \
DATA_PATH="$(pwd)/../../data" \
DEFAULT_VIDEO_PATH=gallery/05_default_juggle.mp4 \
gunicorn \
    --worker-class gthread app:app \
    --workers 1 \
    --threads 2 \
    --bind 0.0.0.0:7263 \
    --timeout 60
```

**Ch·∫°y frontend:**
```bash
cd demo/frontend
yarn install
yarn dev --port 7262
```

## üìä Hi·ªáu su·∫•t c√°c model

| Model | K√≠ch th∆∞·ªõc | T·ªëc ƒë·ªô (FPS) | SA-V test (J&F) | MOSE val (J&F) |
|-------|------------|--------------|------------------|-----------------|
| sam2.1_hiera_tiny | 38.9M | 91.2 | 76.5 | 71.8 |
| sam2.1_hiera_small | 46M | 84.8 | 76.6 | 73.5 |
| sam2.1_hiera_base_plus | 80.8M | 64.1 | 78.2 | 73.7 |
| sam2.1_hiera_large | 224.4M | 39.5 | 79.5 | 74.6 |

## üîß X·ª≠ l√Ω l·ªói th∆∞·ªùng g·∫∑p

### 1. L·ªói CUDA
```bash
# Ki·ªÉm tra CUDA
python -c 'import torch; print(torch.cuda.is_available())'

# Thi·∫øt l·∫≠p CUDA_HOME n·∫øu c·∫ßn
export CUDA_HOME=/usr/local/cuda
```

### 2. L·ªói ImportError
```bash
# C√†i ƒë·∫∑t l·∫°i SAM2
pip uninstall -y SAM-2
pip install -e ".[notebooks]"
```

### 3. L·ªói thi·∫øu config
```bash
# Thi·∫øt l·∫≠p PYTHONPATH
export SAM2_REPO_ROOT=/path/to/sam2
export PYTHONPATH="${SAM2_REPO_ROOT}:${PYTHONPATH}"
```

### 4. L·ªói Visual Studio tr√™n Windows
N·∫øu g·∫∑p l·ªói Visual Studio kh√¥ng t∆∞∆°ng th√≠ch, th√™m flag `-allow-unsupported-compiler` v√†o file `setup.py` t·∫°i d√≤ng 48.

## üéÆ V√≠ d·ª• s·ª≠ d·ª•ng

### Ph√¢n ƒëo·∫°n ƒë·ªëi t∆∞·ª£ng t·ª´ ƒëi·ªÉm click

```python
import cv2
import numpy as np
import torch
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

# Load model
checkpoint = "./checkpoints/sam2.1_hiera_base_plus.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

# Load image
image = cv2.imread("path/to/your/image.jpg")
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Set image
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(image_rgb)
    
    # Click point (x, y) v√† label (1 = foreground, 0 = background)
    input_point = np.array([[x, y]])
    input_label = np.array([1])
    
    # Predict mask
    masks, scores, logits = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=True,
    )
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    best_mask = masks[np.argmax(scores)]
    # Visualize mask...
```

### Tracking ƒë·ªëi t∆∞·ª£ng trong video

```python
import cv2
import torch
from sam2.build_sam import build_sam2_video_predictor

# Load video predictor
checkpoint = "./checkpoints/sam2.1_hiera_base_plus.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

# Load video
video_path = "path/to/your/video.mp4"
cap = cv2.VideoCapture(video_path)

# Initialize state
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    state = predictor.init_state(video_path)
    
    # Add initial prompt
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(
        state, 
        points=[[x, y]], 
        labels=[1]
    )
    
    # Propagate through video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        # Process each frame
        print(f"Frame {frame_idx}: {len(object_ids)} objects tracked")
        # Visualize masks...
```

## üìö T√†i li·ªáu tham kh·∫£o

- [Paper ch√≠nh th·ª©c](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)
- [Demo online](https://sam2.metademolab.com/)
- [Dataset SA-V](https://ai.meta.com/datasets/segment-anything-video)
- [Blog Meta AI](https://ai.meta.com/blog/segment-anything-2)

## ü§ù ƒê√≥ng g√≥p

M·ªçi ƒë√≥ng g√≥p ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n! Vui l√≤ng ƒë·ªçc [CONTRIBUTING.md](CONTRIBUTING.md) ƒë·ªÉ bi·∫øt c√°ch ƒë√≥ng g√≥p.

## üîß X·ª≠ l√Ω l·ªói th∆∞·ªùng g·∫∑p

### 1. L·ªói CUDA
```bash
# Ki·ªÉm tra CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Thi·∫øt l·∫≠p CUDA_HOME n·∫øu c·∫ßn (Windows)
set CUDA_HOME=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1
```

### 2. L·ªói ImportError
```bash
# C√†i ƒë·∫∑t l·∫°i SAM2
pip uninstall -y SAM-2
pip install -e ".[notebooks]"
```

### 3. L·ªói thi·∫øu config
```bash
# Thi·∫øt l·∫≠p PYTHONPATH (Windows)
set SAM2_REPO_ROOT=D:\3.Research\3.VisionTransformer\3.Project\sam2
set PYTHONPATH=%SAM2_REPO_ROOT%;%PYTHONPATH%
```

### 4. L·ªói Visual Studio tr√™n Windows
N·∫øu g·∫∑p l·ªói Visual Studio kh√¥ng t∆∞∆°ng th√≠ch, th√™m flag `-allow-unsupported-compiler` v√†o file `setup.py` t·∫°i d√≤ng 48.

## üìä So s√°nh c√°c model

| Model | K√≠ch th∆∞·ªõc | T·ªëc ƒë·ªô | Ch·∫•t l∆∞·ª£ng | RAM c·∫ßn | GPU c·∫ßn |
|-------|------------|--------|------------|---------|---------|
| sam2.1_hiera_tiny | 38.9M | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 4GB | GTX 1060+ |
| sam2.1_hiera_small | 46M | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 6GB | GTX 1070+ |
| sam2.1_hiera_base_plus | 80.8M | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | RTX 2060+ |
| sam2.1_hiera_large | 224.4M | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 12GB | RTX 3070+ |

**Khuy·∫øn ngh·ªã:**
- **B·∫Øt ƒë·∫ßu**: sam2.1_hiera_tiny (nhanh, √≠t RAM)
- **S·∫£n xu·∫•t**: sam2.1_hiera_base_plus (c√¢n b·∫±ng t·ªët nh·∫•t)
- **Nghi√™n c·ª©u**: sam2.1_hiera_large (ch·∫•t l∆∞·ª£ng cao nh·∫•t)

## üìÑ Gi·∫•y ph√©p

SAM 2 ƒë∆∞·ª£c ph√°t h√†nh d∆∞·ªõi gi·∫•y ph√©p [Apache 2.0](LICENSE).

## üìû H·ªó tr·ª£

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, vui l√≤ng:
1. Ki·ªÉm tra [INSTALL.md](INSTALL.md) ƒë·ªÉ xem c√°c l·ªói th∆∞·ªùng g·∫∑p
2. T·∫°o issue tr√™n GitHub repository
3. Tham kh·∫£o [FAQ](https://github.com/facebookresearch/sam2/issues)

## üéâ K·∫øt lu·∫≠n

B·∫°n ƒë√£ c√≥ h∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ:
- ‚úÖ Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Python v√† c√†i ƒë·∫∑t dependencies
- ‚úÖ C√†i ƒë·∫∑t SAM2 package
- ‚úÖ T·∫£i xu·ªëng c√°c model checkpoints
- ‚úÖ Ch·∫°y c√°c v√≠ d·ª• demo
- ‚úÖ Thi·∫øt l·∫≠p v√† ch·∫°y web demo

SAM2 l√† m·ªôt c√¥ng c·ª• m·∫°nh m·∫Ω cho vi·ªác ph√¢n ƒëo·∫°n ƒë·ªëi t∆∞·ª£ng trong h√¨nh ·∫£nh v√† video. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu v·ªõi c√°c notebook v√≠ d·ª• ƒë·ªÉ l√†m quen v·ªõi API, sau ƒë√≥ t√≠ch h·ª£p v√†o c√°c d·ª± √°n c·ªßa m√¨nh.

**L∆∞u √Ω quan tr·ªçng:**
- S·ª≠ d·ª•ng GPU ƒë·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t
- Model `tiny` ph√π h·ª£p cho th·ª≠ nghi·ªám nhanh
- Model `large` cho ch·∫•t l∆∞·ª£ng cao nh·∫•t
- Web demo cung c·∫•p giao di·ªán tr·ª±c quan ƒë·ªÉ test

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi SAM2! üöÄ**

*H∆∞·ªõng d·∫´n n√†y ƒë∆∞·ª£c t√πy ch·ªânh cho m√°y Windows c·ªßa b·∫°n v·ªõi Python 3.12.0*
