{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v_F6p_xar7t0"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# C·∫§U H√åNH HU·∫§N LUY·ªÜN\n",
        "# =============================================================================\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "BATCH_SIZE=16\n",
        "EARLY_STOPPING_PATIENCE = 10  # D·ª´ng n·∫øu kh√¥ng c·∫£i thi·ªán sau N epochs\n",
        "RESUME_TRAINING = False  # Resume t·ª´ checkpoint n·∫øu c√≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td7-zgg8Xkyl",
        "outputId": "e66134bd-82fa-41be-ac06-23638d04b9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN CHO GOOGLE COLAB\n",
            "======================================================================\n",
            "üì¶ ƒêang c√†i ƒë·∫∑t PyTorch v·ªõi CUDA 11.8...\n",
            "üì¶ ƒêang c√†i ƒë·∫∑t transformers v√† timm...\n",
            "üì¶ ƒêang c√†i ƒë·∫∑t OpenCV, PIL, v√† c√°c th∆∞ vi·ªán x·ª≠ l√Ω ·∫£nh...\n",
            "üì¶ ƒêang c√†i ƒë·∫∑t tqdm cho progress bar...\n",
            "‚úì Ho√†n th√†nh c√†i ƒë·∫∑t th∆∞ vi·ªán!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# C·∫§U H√åNH M√îI TR∆Ø·ªúNG\n",
        "# =============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN CHO GOOGLE COLAB\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"üì¶ ƒêang c√†i ƒë·∫∑t PyTorch v·ªõi CUDA 11.8...\")\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "print(\"üì¶ ƒêang c√†i ƒë·∫∑t transformers v√† timm...\")\n",
        "!pip install -q transformers timm\n",
        "\n",
        "print(\"üì¶ ƒêang c√†i ƒë·∫∑t OpenCV, PIL, v√† c√°c th∆∞ vi·ªán x·ª≠ l√Ω ·∫£nh...\")\n",
        "!pip install -q opencv-python pillow numpy pandas matplotlib seaborn scikit-learn scikit-image\n",
        "\n",
        "print(\"üì¶ ƒêang c√†i ƒë·∫∑t tqdm cho progress bar...\")\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úì Ho√†n th√†nh c√†i ƒë·∫∑t th∆∞ vi·ªán!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpUod-tHsKvc",
        "outputId": "d91b1037-ab13-48b4-ab90-757023d38a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "KI·ªÇM TRA GPU\n",
            "======================================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.6\n",
            "GPU Memory: 14.74 GB\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TH√îNG TIN M√îI TR∆Ø·ªúNG\n",
            "======================================================================\n",
            "‚úì Device: CUDA\n",
            "‚úì PyTorch: 2.8.0+cu126\n",
            "‚úì Transformers: True\n",
            "‚úì TIMM: True\n",
            "‚úì TQDM: True\n",
            "======================================================================\n",
            "‚úì ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán th√†nh c√¥ng!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# IMPORT TH∆Ø VI·ªÜN C∆† B·∫¢N\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT TH∆Ø VI·ªÜN X·ª¨ L√ù ·∫¢NH\n",
        "# =============================================================================\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT TH∆Ø VI·ªÜN VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# C·∫•u h√¨nh matplotlib\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT TH∆Ø VI·ªÜN DEEP LEARNING\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT TH∆Ø VI·ªÜN DEEP LEARNING\n",
        "# =============================================================================\n",
        "\n",
        "# X·ª≠ l√Ω l·ªói import torch (AttributeError: partially initialized module)\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import Dataset, DataLoader, random_split\n",
        "    from torchvision import transforms\n",
        "    import torchvision.transforms.functional as TF\n",
        "except AttributeError as e:\n",
        "    if \"partially initialized module 'torch'\" in str(e) or \"'torch' has no attribute\" in str(e):\n",
        "        print(\"=\"*70)\n",
        "        print(\"‚ö†Ô∏è  L·ªñI: CIRCULAR IMPORT HO·∫∂C CORRUPT TORCH MODULE\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"ƒêang th·ª≠ fix...\")\n",
        "\n",
        "        # X√≥a cache\n",
        "        import sys\n",
        "        modules_to_remove = [k for k in sys.modules.keys() if 'torch' in k]\n",
        "        for module in modules_to_remove:\n",
        "            if 'torch' in module:\n",
        "                del sys.modules[module]\n",
        "\n",
        "        # Reinstall PyTorch\n",
        "        print(\"üì¶ ƒêang reinstall PyTorch...\")\n",
        "        get_ipython().system('pip uninstall -y torch torchvision torchaudio')\n",
        "        get_ipython().system('pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir')\n",
        "\n",
        "        # Th·ª≠ import l·∫°i\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        import torch.optim as optim\n",
        "        from torch.utils.data import Dataset, DataLoader, random_split\n",
        "        from torchvision import transforms\n",
        "        import torchvision.transforms.functional as TF\n",
        "        print(\"‚úì ƒê√£ fix v√† import torch th√†nh c√¥ng!\")\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Ki·ªÉm tra GPU sau khi import torch\n",
        "print(\"=\"*70)\n",
        "print(\"KI·ªÇM TRA GPU\")\n",
        "print(\"=\"*70)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  CUDA kh√¥ng kh·∫£ d·ª•ng! H√£y ƒë·∫£m b·∫£o Runtime -> Change runtime type -> GPU ƒë∆∞·ª£c ch·ªçn\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT VISION TRANSFORMER\n",
        "# =============================================================================\n",
        "\n",
        "# Import transformers\n",
        "try:\n",
        "    from transformers import ViTModel, ViTConfig\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  transformers ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang c√†i...\")\n",
        "    !pip install -q transformers\n",
        "    from transformers import ViTModel, ViTConfig\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "\n",
        "# Import timm\n",
        "try:\n",
        "    import timm\n",
        "    TIMM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  timm ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, ƒëang c√†i...\")\n",
        "    !pip install -q timm\n",
        "    import timm\n",
        "    TIMM_AVAILABLE = True\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORT METRICS V√Ä UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "from scipy import ndimage\n",
        "\n",
        "# Progress bar\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    from tqdm.notebook import tqdm as tqdm_notebook\n",
        "    USE_TQDM = True\n",
        "except ImportError:\n",
        "    USE_TQDM = False\n",
        "    print(\"‚ö†Ô∏è  tqdm ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "\n",
        "# =============================================================================\n",
        "# HI·ªÇN TH·ªä TH√îNG TIN M√îI TR∆Ø·ªúNG\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TH√îNG TIN M√îI TR∆Ø·ªúNG\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úì Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"‚úì PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úì Transformers: {TRANSFORMERS_AVAILABLE}\")\n",
        "print(f\"‚úì TIMM: {TIMM_AVAILABLE}\")\n",
        "print(f\"‚úì TQDM: {USE_TQDM}\")\n",
        "print(\"=\"*70)\n",
        "print(\"‚úì ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán th√†nh c√¥ng!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FzPmazSaO1m"
      },
      "source": [
        "1.1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzw5YmtzsSke",
        "outputId": "0ae7411c-90d6-4d01-943f-b6039fe9e7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "C·∫§U H√åNH GOOGLE DRIVE\n",
            "======================================================================\n",
            "Mounted at /content/drive\n",
            "\n",
            "üìÅ Ch·ªçn m·ªôt trong c√°c ph∆∞∆°ng √°n sau:\n",
            "   1. D·ªØ li·ªáu tr√™n Google Drive: /content/drive/MyDrive/ViT/datasets-sam-vit\n",
            "   2. Upload tr·ª±c ti·∫øp l√™n Colab: /content/ViT/datasets-sam-vit\n",
            "‚úì T√¨m th·∫•y d·ªØ li·ªáu tr√™n Google Drive: /content/drive/MyDrive/ViT/datasets-sam-vit\n",
            "\n",
            "======================================================================\n",
            "KI·ªÇM TRA ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU\n",
            "======================================================================\n",
            "BASE_DATA_DIR: /content/drive/MyDrive/ViT/datasets-sam-vit\n",
            "IMAGES_DIR: /content/drive/MyDrive/ViT/datasets-sam-vit/images (‚úì)\n",
            "MASKS_DIR: /content/drive/MyDrive/ViT/datasets-sam-vit/masks (‚úì)\n",
            "LABELS_DIR: /content/drive/MyDrive/ViT/datasets-sam-vit/labels (‚úó)\n",
            "\n",
            "S·ªë classes: 3\n",
            "Class mapping:\n",
            "  1: brown_planthopper\n",
            "  2: whitebacked_planthopper\n",
            "  3: rice_leaf_miner\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"C·∫§U H√åNH GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "print(\"\\nüìÅ Ch·ªçn m·ªôt trong c√°c ph∆∞∆°ng √°n sau:\")\n",
        "print(\"   1. D·ªØ li·ªáu tr√™n Google Drive: /content/drive/MyDrive/ViT/datasets-sam-vit\")\n",
        "print(\"   2. Upload tr·ª±c ti·∫øp l√™n Colab: /content/ViT/datasets-sam-vit\")\n",
        "\n",
        "# T·ª± ƒë·ªông ph√°t hi·ªán ƒë∆∞·ªùng d·∫´n\n",
        "drive_path = Path(\"/content/drive/MyDrive/ViT/datasets-sam-vit\")\n",
        "\n",
        "if drive_path.exists() and (drive_path / \"images\").exists():\n",
        "    BASE_DATA_DIR = drive_path\n",
        "    print(f\"‚úì T√¨m th·∫•y d·ªØ li·ªáu tr√™n Google Drive: {BASE_DATA_DIR}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu!\")\n",
        "\n",
        "# C√°c th∆∞ m·ª•c con\n",
        "IMAGES_DIR = BASE_DATA_DIR / \"images\"\n",
        "MASKS_DIR = BASE_DATA_DIR / \"masks\"\n",
        "\n",
        "# Mapping class IDs (t·ª´ mask values sang t√™n class)\n",
        "CLASS_MAPPING = {\n",
        "    0: \"background\",\n",
        "    1: \"brown_planthopper\",      # R·∫ßy n√¢u (BPH) - Category ID 1\n",
        "    2: \"whitebacked_planthopper\", # R·∫ßy l∆∞ng tr·∫Øng (WBPH) - Category ID 2\n",
        "    3: \"rice_leaf_miner\"         # S√¢u ƒÉn l√° l√∫a (RLM) - Category ID 3\n",
        "}\n",
        "\n",
        "NUM_CLASSES = len([k for k in CLASS_MAPPING.keys() if k > 0])  # B·ªè background: 3 classes\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"KI·ªÇM TRA ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"BASE_DATA_DIR: {BASE_DATA_DIR}\")\n",
        "print(f\"IMAGES_DIR: {IMAGES_DIR} ({'‚úì' if IMAGES_DIR.exists() else '‚úó'})\")\n",
        "print(f\"MASKS_DIR: {MASKS_DIR} ({'‚úì' if MASKS_DIR.exists() else '‚úó'})\")\n",
        "print(f\"\\nS·ªë classes: {NUM_CLASSES}\")\n",
        "print(f\"Class mapping:\")\n",
        "for k, v in CLASS_MAPPING.items():\n",
        "    if k > 0:\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "if not IMAGES_DIR.exists() or not MASKS_DIR.exists():\n",
        "    print(f\"\\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Th∆∞ m·ª•c d·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i!\")\n",
        "    print(\"   Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n ho·∫∑c upload d·ªØ li·ªáu.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShdzhI5nsiNu"
      },
      "source": [
        "## 1.2. ƒê·ªçc v√† ph√¢n t√≠ch d·ªØ li·ªáu (images v√† masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGasQc8usj48"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset(images_dir: Path, masks_dir: Path, splits: List[str] = ['train', 'val', 'test']):\n",
        "    \"\"\"\n",
        "    Ph√¢n t√≠ch dataset: ƒë·∫øm s·ªë l∆∞·ª£ng ·∫£nh, masks, classes theo t·ª´ng split\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for split in splits:\n",
        "        split_images_dir = images_dir / split\n",
        "        split_masks_dir = masks_dir / split\n",
        "\n",
        "        if not split_images_dir.exists() or not split_masks_dir.exists():\n",
        "            print(f\"‚ö†Ô∏è  Th∆∞ m·ª•c {split} kh√¥ng t·ªìn t·∫°i, b·ªè qua...\")\n",
        "            continue\n",
        "\n",
        "        # T√¨m t·∫•t c·∫£ file ·∫£nh\n",
        "        image_files = list(split_images_dir.glob(\"*.jpg\")) + list(split_images_dir.glob(\"*.JPG\")) + \\\n",
        "                     list(split_images_dir.glob(\"*.png\")) + list(split_images_dir.glob(\"*.PNG\"))\n",
        "\n",
        "        # ƒê·∫øm masks v√† classes\n",
        "        mask_files = list(split_masks_dir.glob(\"*.png\"))\n",
        "\n",
        "        # ƒê·∫øm classes trong masks\n",
        "        class_counts = Counter()\n",
        "        total_mask_pixels = 0\n",
        "        image_sizes = []\n",
        "        valid_pairs = 0\n",
        "\n",
        "        for img_file in image_files:\n",
        "            mask_file = split_masks_dir / f\"{img_file.stem}.png\"\n",
        "            if mask_file.exists():\n",
        "                valid_pairs += 1\n",
        "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
        "                if mask is not None:\n",
        "                    unique_classes = np.unique(mask)\n",
        "                    for cls in unique_classes:\n",
        "                        if cls > 0:  # B·ªè background\n",
        "                            class_counts[cls] += np.sum(mask == cls)\n",
        "                            total_mask_pixels += np.sum(mask == cls)\n",
        "\n",
        "                    # ƒê·ªçc k√≠ch th∆∞·ªõc ·∫£nh\n",
        "                    img = cv2.imread(str(img_file))\n",
        "                    if img is not None:\n",
        "                        h, w = img.shape[:2]\n",
        "                        image_sizes.append((w, h))\n",
        "\n",
        "        results[split] = {\n",
        "            'total_images': len(image_files),\n",
        "            'total_masks': len(mask_files),\n",
        "            'valid_pairs': valid_pairs,\n",
        "            'class_counts': dict(class_counts),\n",
        "            'total_mask_pixels': total_mask_pixels,\n",
        "            'image_sizes': image_sizes\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Split: {split.upper()}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"T·ªïng s·ªë ·∫£nh: {results[split]['total_images']}\")\n",
        "        print(f\"T·ªïng s·ªë masks: {results[split]['total_masks']}\")\n",
        "        print(f\"S·ªë c·∫∑p ·∫£nh-mask h·ª£p l·ªá: {results[split]['valid_pairs']}\")\n",
        "        print(f\"Ph√¢n b·ªë classes (pixels):\")\n",
        "        for cls_id, count in sorted(results[split]['class_counts'].items()):\n",
        "            cls_name = CLASS_MAPPING.get(cls_id, f\"Unknown_{cls_id}\")\n",
        "            percentage = (count / total_mask_pixels * 100) if total_mask_pixels > 0 else 0\n",
        "            print(f\"  - {cls_name} (ID={cls_id}): {count:,} pixels ({percentage:.2f}%)\")\n",
        "\n",
        "        if image_sizes:\n",
        "            sizes_array = np.array(image_sizes)\n",
        "            print(f\"\\nK√≠ch th∆∞·ªõc ·∫£nh (W x H):\")\n",
        "            print(f\"  - Trung b√¨nh: {sizes_array[:, 0].mean():.0f} x {sizes_array[:, 1].mean():.0f}\")\n",
        "            print(f\"  - Min: {sizes_array[:, 0].min()} x {sizes_array[:, 1].min()}\")\n",
        "            print(f\"  - Max: {sizes_array[:, 0].max()} x {sizes_array[:, 1].max()}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Ph√¢n t√≠ch dataset\n",
        "dataset_stats = analyze_dataset(IMAGES_DIR, MASKS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnToMdlSsncx"
      },
      "source": [
        "## 1.3. Visualize m·ªôt s·ªë m·∫´u d·ªØ li·ªáu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l84RR2u3sn8E"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(images_dir: Path, masks_dir: Path, split: str = 'train', num_samples: int = 6):\n",
        "    \"\"\"\n",
        "    Hi·ªÉn th·ªã m·ªôt s·ªë m·∫´u ·∫£nh v√† mask t∆∞∆°ng ·ª©ng\n",
        "    \"\"\"\n",
        "    split_images_dir = images_dir / split\n",
        "    split_masks_dir = masks_dir / split\n",
        "\n",
        "    if not split_images_dir.exists() or not split_masks_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  Th∆∞ m·ª•c {split} kh√¥ng t·ªìn t·∫°i\")\n",
        "        return\n",
        "\n",
        "    # T√¨m c√°c file ·∫£nh c√≥ mask t∆∞∆°ng ·ª©ng\n",
        "    image_files = list(split_images_dir.glob(\"*.jpg\")) + list(split_images_dir.glob(\"*.png\"))\n",
        "    valid_pairs = []\n",
        "\n",
        "    for img_file in image_files[:50]:  # Ch·ªâ ki·ªÉm tra 50 file ƒë·∫ßu\n",
        "        mask_file = split_masks_dir / f\"{img_file.stem}.png\"\n",
        "        if mask_file.exists():\n",
        "            valid_pairs.append((img_file, mask_file))\n",
        "        if len(valid_pairs) >= num_samples:\n",
        "            break\n",
        "\n",
        "    if len(valid_pairs) == 0:\n",
        "        print(f\"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·∫∑p ·∫£nh-mask h·ª£p l·ªá trong split {split}\")\n",
        "        return\n",
        "\n",
        "    # T·∫°o figure\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    # Color map cho mask visualization (RGB)\n",
        "    mask_color_map = {\n",
        "        0: (0, 0, 0),       # Background - ƒëen\n",
        "        1: (255, 0, 0),     # Brown Planthopper - ƒë·ªè\n",
        "        2: (0, 255, 0),     # White-Backed Planthopper - xanh l√°\n",
        "        3: (0, 0, 255),     # Rice Leaf Miner - xanh d∆∞∆°ng\n",
        "    }\n",
        "\n",
        "    for idx, (img_file, mask_file) in enumerate(valid_pairs[:num_samples]):\n",
        "        # ƒê·ªçc ·∫£nh\n",
        "        img = cv2.imread(str(img_file))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # ƒê·ªçc mask\n",
        "        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # T·∫°o mask visualization m√†u\n",
        "        mask_vis = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
        "        for cls_id, color in mask_color_map.items():\n",
        "            mask_vis[mask == cls_id] = color\n",
        "\n",
        "        # Overlay mask l√™n ·∫£nh\n",
        "        overlay = img_rgb.copy()\n",
        "        mask_colored = mask_vis.copy()\n",
        "        mask_colored[mask == 0] = 0  # Ch·ªâ overlay c√°c pixel kh√¥ng ph·∫£i background\n",
        "        overlay = cv2.addWeighted(overlay, 0.7, mask_colored, 0.3, 0)\n",
        "\n",
        "        # Hi·ªÉn th·ªã\n",
        "        axes[idx, 0].imshow(img_rgb)\n",
        "        axes[idx, 0].set_title(f\"·∫¢nh g·ªëc: {img_file.name}\")\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(mask_vis)\n",
        "        axes[idx, 1].set_title(f\"Mask (ID values)\")\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Hi·ªÉn th·ªã c√°c classes c√≥ trong mask\n",
        "        unique_classes = [c for c in np.unique(mask) if c > 0]\n",
        "        classes_text = \", \".join([f\"{CLASS_MAPPING[c]}({c})\" for c in unique_classes])\n",
        "\n",
        "        axes[idx, 2].imshow(overlay)\n",
        "        axes[idx, 2].set_title(f\"Overlay\\nClasses: {classes_text}\")\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "visualize_samples(IMAGES_DIR, MASKS_DIR, split='train', num_samples=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YID1N0d4sp8B"
      },
      "source": [
        "## 1.4. Dataset class ƒë·ªÉ load d·ªØ li·ªáu cho ViT\n",
        "\n",
        "Dataset n√†y s·∫Ω:\n",
        "- ƒê·ªçc ·∫£nh g·ªëc v√† mask t∆∞∆°ng ·ª©ng\n",
        "- Tr√≠ch xu·∫•t c√°c v√πng ROI t·ª´ mask (SAM ƒë√£ t·∫°o mask ch√≠nh x√°c)\n",
        "- Crop v√† resize c√°c ROI v·ªÅ k√≠ch th∆∞·ªõc chu·∫©n cho ViT\n",
        "- √Åp d·ª•ng augmentation n·∫øu c·∫ßn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m7ayqDssreQ"
      },
      "outputs": [],
      "source": [
        "class PlanthopperDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class cho ph√¢n lo·∫°i r·∫ßy n√¢u, r·∫ßy l∆∞ng tr·∫Øng v√† s√¢u ƒÉn l√° l√∫a\n",
        "    ƒê·ªçc ·∫£nh ƒë√£ ƒë∆∞·ª£c mask (t·ª´ SAM2) v√† tr√≠ch xu·∫•t ROI cho ViT\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir: Path,\n",
        "        masks_dir: Path,\n",
        "        split: str,\n",
        "        image_size: int = 224,\n",
        "        augment: bool = False,\n",
        "        extract_roi: bool = True,\n",
        "        min_roi_size: int = 32,\n",
        "    ):\n",
        "        self.images_dir = images_dir / split\n",
        "        self.masks_dir = masks_dir / split\n",
        "        self.split = split\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment and (split == 'train')\n",
        "        self.extract_roi = extract_roi\n",
        "        self.min_roi_size = min_roi_size\n",
        "\n",
        "        # T√¨m t·∫•t c·∫£ c√°c c·∫∑p ·∫£nh-mask h·ª£p l·ªá\n",
        "        image_files = list(self.images_dir.glob(\"*.jpg\")) + list(self.images_dir.glob(\"*.png\")) + \\\n",
        "                     list(self.images_dir.glob(\"*.JPG\")) + list(self.images_dir.glob(\"*.PNG\"))\n",
        "\n",
        "        self.samples = []\n",
        "        for img_file in image_files:\n",
        "            mask_file = self.masks_dir / f\"{img_file.stem}.png\"\n",
        "            if mask_file.exists():\n",
        "                # Ki·ªÉm tra mask c√≥ d·ªØ li·ªáu kh√¥ng\n",
        "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
        "                if mask is not None and np.sum(mask > 0) > 0:\n",
        "                    if extract_roi:\n",
        "                        # Tr√≠ch xu·∫•t t·ª´ng ROI ri√™ng bi·ªát\n",
        "                        rois = self._extract_rois(img_file, mask_file, mask)\n",
        "                        self.samples.extend(rois)\n",
        "                    else:\n",
        "                        # S·ª≠ d·ª•ng to√†n b·ªô ·∫£nh\n",
        "                        self.samples.append({\n",
        "                            'image_file': img_file,\n",
        "                            'mask_file': mask_file,\n",
        "                            'roi_bbox': None,  # To√†n b·ªô ·∫£nh\n",
        "                            'label': None  # S·∫Ω x√°c ƒë·ªãnh t·ª´ mask\n",
        "                        })\n",
        "\n",
        "        print(f\"‚úì Split {split}: {len(self.samples)} samples\")\n",
        "\n",
        "        # Augmentation transforms\n",
        "        if self.augment:\n",
        "            self.aug_transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            ])\n",
        "        else:\n",
        "            self.aug_transform = None\n",
        "\n",
        "        # Normalization (ImageNet mean/std)\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "    def _extract_rois(self, image_file: Path, mask_file: Path, mask: np.ndarray) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Tr√≠ch xu·∫•t c√°c ROI (Regions of Interest) t·ª´ mask\n",
        "        M·ªói ROI t∆∞∆°ng ·ª©ng v·ªõi m·ªôt instance c·ªßa m·ªôt class\n",
        "        \"\"\"\n",
        "        rois = []\n",
        "        unique_classes = [c for c in np.unique(mask) if c > 0]  # B·ªè background\n",
        "\n",
        "        for class_id in unique_classes:\n",
        "            # T·∫°o binary mask cho class n√†y\n",
        "            class_mask = (mask == class_id).astype(np.uint8)\n",
        "\n",
        "            # T√¨m contours ƒë·ªÉ t√°ch c√°c instance ri√™ng bi·ªát\n",
        "            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if area < self.min_roi_size * self.min_roi_size:  # B·ªè qua ROI qu√° nh·ªè\n",
        "                    continue\n",
        "\n",
        "                # L·∫•y bounding box\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "                # M·ªü r·ªông bbox m·ªôt ch√∫t ƒë·ªÉ c√≥ context\n",
        "                padding = 10\n",
        "                x = max(0, x - padding)\n",
        "                y = max(0, y - padding)\n",
        "                w = min(mask.shape[1] - x, w + 2 * padding)\n",
        "                h = min(mask.shape[0] - y, h + 2 * padding)\n",
        "\n",
        "                rois.append({\n",
        "                    'image_file': image_file,\n",
        "                    'mask_file': mask_file,\n",
        "                    'roi_bbox': (x, y, w, h),\n",
        "                    'label': class_id - 1  # Chuy·ªÉn t·ª´ [1,2,3] sang [0,1,2] cho ViT\n",
        "                })\n",
        "\n",
        "        return rois\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # ƒê·ªçc ·∫£nh\n",
        "        img = cv2.imread(str(sample['image_file']))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if sample['roi_bbox'] is not None:\n",
        "            # Crop ROI\n",
        "            x, y, w, h = sample['roi_bbox']\n",
        "            roi = img_rgb[y:y+h, x:x+w]\n",
        "        else:\n",
        "            # S·ª≠ d·ª•ng to√†n b·ªô ·∫£nh\n",
        "            roi = img_rgb\n",
        "\n",
        "        # Resize v·ªÅ image_size\n",
        "        roi_pil = Image.fromarray(roi)\n",
        "        roi_resized = roi_pil.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "\n",
        "        # Convert to tensor\n",
        "        img_tensor = TF.to_tensor(roi_resized)\n",
        "\n",
        "        # Augmentation\n",
        "        if self.aug_transform is not None:\n",
        "            img_tensor = self.aug_transform(img_tensor)\n",
        "\n",
        "        # Normalize\n",
        "        img_tensor = self.normalize(img_tensor)\n",
        "\n",
        "        # Label\n",
        "        label = sample['label']\n",
        "        if label is None:\n",
        "            # X√°c ƒë·ªãnh label t·ª´ mask n·∫øu ch∆∞a c√≥\n",
        "            mask = cv2.imread(str(sample['mask_file']), cv2.IMREAD_GRAYSCALE)\n",
        "            unique_classes = [c for c in np.unique(mask) if c > 0]\n",
        "            label = unique_classes[0] - 1 if unique_classes else 0\n",
        "\n",
        "        return img_tensor, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Test dataset\n",
        "print(\"T·∫°o datasets...\")\n",
        "train_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'train', augment=True)\n",
        "val_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'val', augment=False)\n",
        "test_dataset = PlanthopperDataset(IMAGES_DIR, MASKS_DIR, 'test', augment=False)\n",
        "\n",
        "print(f\"\\nTrain: {len(train_dataset)} samples\")\n",
        "print(f\"Val: {len(val_dataset)} samples\")\n",
        "print(f\"Test: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vibay8dfsvNZ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD SAM2 MODEL (Optional - ch·ªâ c·∫ßn n·∫øu mu·ªën t·∫°o masks m·ªõi)\n",
        "# =============================================================================\n",
        "\n",
        "LOAD_SAM = False  # ƒê·∫∑t True n·∫øu c·∫ßn s·ª≠ d·ª•ng SAM\n",
        "\n",
        "if LOAD_SAM:\n",
        "    from sam2.build_sam import build_sam2\n",
        "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "    # ƒê∆∞·ªùng d·∫´n checkpoint v√† config\n",
        "    SAM_CHECKPOINT = \"./checkpoints/sam2.1_hiera_tiny.pt\"\n",
        "    SAM_MODEL_CFG = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
        "\n",
        "    if os.path.exists(SAM_CHECKPOINT):\n",
        "        print(\"Loading SAM2 model...\")\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        sam_model = build_sam2(SAM_MODEL_CFG, SAM_CHECKPOINT, device=device)\n",
        "        sam_predictor = SAM2ImagePredictor(sam_model)\n",
        "        print(f\"‚úì SAM2 loaded on {device}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  SAM checkpoint not found at {SAM_CHECKPOINT}\")\n",
        "        print(\"   Skip loading SAM (masks ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn)\")\n",
        "        sam_predictor = None\n",
        "else:\n",
        "    print(\"Skip loading SAM (masks ƒë√£ ƒë∆∞·ª£c t·∫°o s·∫µn t·ª´ b∆∞·ªõc preprocessing)\")\n",
        "    sam_predictor = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA8em20esx3C"
      },
      "source": [
        "## 2.2. X√¢y d·ª±ng Vision Transformer (ViT) Model\n",
        "\n",
        "S·ª≠ d·ª•ng ViT ƒë·ªÉ ph√¢n lo·∫°i c√°c ROI ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ SAM masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFnnUzgyszB8"
      },
      "outputs": [],
      "source": [
        "class ViTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer classifier cho ph√¢n lo·∫°i r·∫ßy\n",
        "    S·ª≠ d·ª•ng pretrained ViT v√† th√™m classification head\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 3,\n",
        "        model_name: str = 'google/vit-base-patch16-224',\n",
        "        pretrained: bool = True,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ViT\n",
        "        if pretrained:\n",
        "            self.vit = ViTModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            config = ViTConfig.from_pretrained(model_name)\n",
        "            self.vit = ViTModel(config)\n",
        "\n",
        "        # Feature dimension\n",
        "        hidden_size = self.vit.config.hidden_size\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # Freeze m·ªôt ph·∫ßn encoder n·∫øu mu·ªën (optional)\n",
        "        # for param in list(self.vit.encoder.layer[:6].parameters()):\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ViT forward\n",
        "        outputs = self.vit(pixel_values=x)\n",
        "\n",
        "        # L·∫•y [CLS] token\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(cls_token)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Ho·∫∑c s·ª≠ d·ª•ng timm (alternative)\n",
        "class ViTClassifierTimm(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT classifier s·ª≠ d·ª•ng timm library\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 3,\n",
        "        model_name: str = 'vit_base_patch16_224',\n",
        "        pretrained: bool = True,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ViT t·ª´ timm\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,  # Ch·ªâ l·∫•y features\n",
        "        )\n",
        "\n",
        "        feature_dim = self.backbone.num_features\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim, feature_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# T·∫°o model\n",
        "print(\"T·∫°o ViT model...\")\n",
        "model = ViTClassifierTimm(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    model_name='vit_base_patch16_224',  # C√≥ th·ªÉ thay b·∫±ng 'vit_small_patch16_224' cho model nh·ªè h∆°n\n",
        "    pretrained=True,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Test forward pass\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_input)\n",
        "print(f\"‚úì Model created!\")\n",
        "print(f\"  Input shape: {dummy_input.shape}\")\n",
        "print(f\"  Output shape: {output.shape}\")\n",
        "print(f\"  Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd7Dk-ors2bg"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# T·∫†O DATALOADERS\n",
        "# =============================================================================\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"‚úì DataLoaders created!\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Ki·ªÉm tra m·ªôt batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "images, labels = sample_batch\n",
        "print(f\"\\nSample batch:\")\n",
        "print(f\"  Images shape: {images.shape}\")\n",
        "print(f\"  Labels shape: {labels.shape}\")\n",
        "print(f\"  Labels: {labels[:10].tolist()}...\")  # Hi·ªÉn th·ªã 10 labels ƒë·∫ßu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMJ0DwVps30m"
      },
      "source": [
        "## 3.2. H√†m hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNf8wNQHs5N2"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Hu·∫•n luy·ªán m·ªôt epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Progress bar\n",
        "    if USE_TQDM:\n",
        "        pbar = tqdm_notebook(train_loader, desc=f'Epoch {epoch+1} [Train]', leave=False)\n",
        "    else:\n",
        "        pbar = train_loader\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(pbar):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        # Forward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping ƒë·ªÉ tr√°nh gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        if USE_TQDM:\n",
        "            current_acc = 100. * correct / total\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{current_acc:.2f}%'\n",
        "            })\n",
        "        elif (batch_idx + 1) % 50 == 0:\n",
        "            print(f'  Batch [{batch_idx+1}/{len(train_loader)}], '\n",
        "                  f'Loss: {loss.item():.4f}, '\n",
        "                  f'Acc: {100.*correct/total:.2f}%')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device, class_names=None, desc='Eval'):\n",
        "    \"\"\"ƒê√°nh gi√° model tr√™n validation/test set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Progress bar\n",
        "    if USE_TQDM:\n",
        "        pbar = tqdm_notebook(data_loader, desc=desc, leave=False)\n",
        "    else:\n",
        "        pbar = data_loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            if USE_TQDM:\n",
        "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # T√≠nh c√°c metrics\n",
        "    accuracy = 100. * np.mean(all_preds == all_labels)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class_{i}\" for i in range(len(np.unique(all_labels)))]\n",
        "\n",
        "    unique_labels = np.unique(all_labels)\n",
        "    for cls_id in unique_labels:\n",
        "        cls_mask = all_labels == cls_id\n",
        "        if np.sum(cls_mask) > 0:\n",
        "            cls_acc = 100. * np.mean(all_preds[cls_mask] == all_labels[cls_mask])\n",
        "            cls_precision = precision_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)\n",
        "            cls_recall = recall_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)\n",
        "            cls_f1 = f1_score(all_labels == cls_id, all_preds == cls_id, zero_division=0)\n",
        "\n",
        "            per_class_metrics[class_names[cls_id]] = {\n",
        "                'accuracy': cls_acc,\n",
        "                'precision': cls_precision,\n",
        "                'recall': cls_recall,\n",
        "                'f1': cls_f1\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'per_class': per_class_metrics\n",
        "    }\n",
        "\n",
        "print(\"‚úì Training v√† evaluation functions ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kayYeaZYs7-D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{'='*70}\")\n",
        "print(\"C·∫§U H√åNH HU·∫§N LUY·ªÜN\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Di chuy·ªÉn model l√™n device\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function (CrossEntropy v·ªõi class weights ƒë·ªÉ x·ª≠ l√Ω class imbalance)\n",
        "# T√≠nh class weights t·ª´ dataset\n",
        "print(\"\\nüìä T√≠nh to√°n class weights...\")\n",
        "class_counts = np.zeros(NUM_CLASSES)\n",
        "print(\"   ƒêang ƒë·∫øm samples trong train dataset...\")\n",
        "for _, label in train_dataset:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "total_samples = np.sum(class_counts)\n",
        "class_weights = total_samples / (NUM_CLASSES * class_counts)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "print(f\"   Class counts: {class_counts}\")\n",
        "print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "print(f\"   Total samples: {total_samples}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning rate scheduler (CosineAnnealingLR v·ªõi warmup)\n",
        "warmup_epochs = 3\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=NUM_EPOCHS - warmup_epochs,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "# L∆∞u training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'val_f1': [],\n",
        "    'learning_rates': []\n",
        "}\n",
        "\n",
        "# Best model\n",
        "best_val_f1 = 0.0\n",
        "best_epoch = 0\n",
        "best_model_state = None\n",
        "early_stopping_counter = 0\n",
        "\n",
        "# Resume training n·∫øu c·∫ßn\n",
        "start_epoch = 0\n",
        "if RESUME_TRAINING:\n",
        "    checkpoint_path = Path(\"/content/drive/MyDrive/ViT_checkpoints/vit_classifier_best.pth\")\n",
        "    if checkpoint_path.exists():\n",
        "        print(f\"\\nüìÇ Resume training t·ª´ checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        best_val_f1 = checkpoint.get('best_val_f1', 0.0)\n",
        "        history = checkpoint.get('history', history)\n",
        "        print(f\"   Resume t·ª´ epoch {start_epoch}, best F1: {best_val_f1:.4f}\")\n",
        "\n",
        "class_names = [CLASS_MAPPING[i+1] for i in range(NUM_CLASSES)]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Start epoch: {start_epoch}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WuC9y--s9zm"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING LOOP\n",
        "# =============================================================================\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Warmup learning rate cho 3 epochs ƒë·∫ßu\n",
        "    if epoch < warmup_epochs:\n",
        "        warmup_lr = LEARNING_RATE * (epoch + 1) / warmup_epochs\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = warmup_lr\n",
        "    else:\n",
        "        scheduler.step()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    history['learning_rates'].append(current_lr)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "\n",
        "    # Validate\n",
        "    val_results = evaluate(model, val_loader, criterion, device, class_names=class_names, desc=f'Epoch {epoch+1} [Val]')\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_results['loss'])\n",
        "    history['val_acc'].append(val_results['accuracy'])\n",
        "    history['val_f1'].append(val_results['f1'])\n",
        "\n",
        "    # Print results\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f\"\\nTrain - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val   - Loss: {val_results['loss']:.4f}, \"\n",
        "          f\"Acc: {val_results['accuracy']:.2f}%, \"\n",
        "          f\"F1: {val_results['f1']:.4f}\")\n",
        "    print(f\"      - Precision: {val_results['precision']:.4f}, \"\n",
        "          f\"Recall: {val_results['recall']:.4f}\")\n",
        "    print(f\"      - LR: {current_lr:.6f}, Time: {epoch_time:.1f}s\")\n",
        "\n",
        "    # Print per-class metrics\n",
        "    print(\"\\n      Per-class metrics:\")\n",
        "    for cls_name, metrics in val_results['per_class'].items():\n",
        "        print(f\"        {cls_name}: \"\n",
        "              f\"Acc={metrics['accuracy']:.2f}%, \"\n",
        "              f\"P={metrics['precision']:.4f}, \"\n",
        "              f\"R={metrics['recall']:.4f}, \"\n",
        "              f\"F1={metrics['f1']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    improved = False\n",
        "    if val_results['f1'] > best_val_f1:\n",
        "        best_val_f1 = val_results['f1']\n",
        "        best_epoch = epoch + 1\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        early_stopping_counter = 0\n",
        "        improved = True\n",
        "        print(f\"\\n      ‚úÖ New best F1: {best_val_f1:.4f} (Epoch {best_epoch})\")\n",
        "\n",
        "        # L∆∞u checkpoint tr√™n Colab\n",
        "        checkpoint_dir = Path(\"/content/drive/MyDrive/ViT_checkpoints\")\n",
        "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'best_val_f1': best_val_f1,\n",
        "            'best_epoch': best_epoch,\n",
        "            'test_results': val_results,\n",
        "            'class_names': class_names,\n",
        "            'history': history,\n",
        "            'config': {\n",
        "                'num_classes': NUM_CLASSES,\n",
        "                'learning_rate': LEARNING_RATE,\n",
        "                'batch_size': BATCH_SIZE,\n",
        "                'class_mapping': CLASS_MAPPING\n",
        "            }\n",
        "        }\n",
        "\n",
        "        checkpoint_path = checkpoint_dir / 'vit_classifier_best.pth'\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"      üíæ Saved checkpoint: {checkpoint_path}\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        print(f\"\\n      ‚è≥ No improvement ({early_stopping_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"‚èπÔ∏è  EARLY STOPPING\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Kh√¥ng c·∫£i thi·ªán sau {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "        print(f\"Best F1: {best_val_f1:.4f} t·∫°i epoch {best_epoch}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"\\n‚úì Loaded best model (F1: {best_val_f1:.4f} t·∫°i epoch {best_epoch})\")\n",
        "\n",
        "total_training_time = time.time() - training_start_time\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"HU·∫§N LUY·ªÜN HO√ÄN T·∫§T\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total training time: {total_training_time/60:.1f} minutes\")\n",
        "print(f\"Best validation F1: {best_val_f1:.4f}\")\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7bd7ahws_2e"
      },
      "source": [
        "## 3.4. Visualize training history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r815t4YftArP"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω ƒë·ªì th·ªã training history\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "# F1 Score\n",
        "axes[2].plot(history['val_f1'], label='Val F1', marker='s', color='green')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('F1 Score')\n",
        "axes[2].set_title('Validation F1 Score')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K85-ml-tCeC"
      },
      "source": [
        "## 3.5. ƒê√°nh gi√° tr√™n test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKGYmWH9tDR7"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° tr√™n test set\n",
        "print(\"=\"*70)\n",
        "print(\"ƒê√ÅNH GI√Å TR√äN TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_results = evaluate(model, test_loader, criterion, device, class_names=class_names, desc='Test')\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Loss: {test_results['loss']:.4f}\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1 Score: {test_results['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nPer-class metrics:\")\n",
        "for cls_name, metrics in test_results['per_class'].items():\n",
        "    print(f\"  {cls_name}:\")\n",
        "    print(f\"    Accuracy: {metrics['accuracy']:.2f}%\")\n",
        "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"    Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"    F1: {metrics['f1']:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    test_results['labels'],\n",
        "    test_results['predictions'],\n",
        "    target_names=class_names\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_results['labels'], test_results['predictions'])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix - Test Set (SAM-ViT)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emnjeiWCtFOs"
      },
      "source": [
        "# 4. PH√ÇN T√çCH K·∫æT QU·∫¢, X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò V√Ä SO S√ÅNH\n",
        "\n",
        "## 4.1. T√≠nh IoU v√† Dice Coefficient cho Segmentation Masks\n",
        "\n",
        "T√≠nh to√°n IoU (Intersection over Union) v√† Dice coefficient ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng segmentation masks t·ª´ SAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6b5lp3FtGmE"
      },
      "outputs": [],
      "source": [
        "# Visualize m·ªôt s·ªë m·∫´u d·ª± ƒëo√°n\n",
        "def visualize_predictions(model, test_loader, device, num_samples=12):\n",
        "    \"\"\"Hi·ªÉn th·ªã m·ªôt s·ªë m·∫´u d·ª± ƒëo√°n tr√™n test set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # L·∫•y m·ªôt batch t·ª´ test_loader\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "    images = images.cpu()\n",
        "    labels = labels.cpu().numpy()\n",
        "    predicted = predicted.cpu().numpy()\n",
        "    probabilities = probabilities.cpu().numpy()\n",
        "\n",
        "    # T·∫°o figure\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    class_names = [CLASS_MAPPING[i+1] for i in range(NUM_CLASSES)]\n",
        "\n",
        "    for idx in range(min(num_samples, len(images))):\n",
        "        img = images[idx]\n",
        "        true_label = labels[idx]\n",
        "        pred_label = predicted[idx]\n",
        "        prob = probabilities[idx]\n",
        "\n",
        "        # Denormalize image\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        img_denorm = img * std + mean\n",
        "        img_denorm = torch.clamp(img_denorm, 0, 1)\n",
        "        img_np = img_denorm.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Hi·ªÉn th·ªã\n",
        "        axes[idx].imshow(img_np)\n",
        "\n",
        "        # Title v·ªõi th√¥ng tin prediction\n",
        "        true_name = class_names[true_label]\n",
        "        pred_name = class_names[pred_label]\n",
        "        correct = \"‚úì\" if true_label == pred_label else \"‚úó\"\n",
        "\n",
        "        title = f\"{correct} True: {true_name}\\nPred: {pred_name} ({prob[pred_label]:.2f})\"\n",
        "        color = 'green' if true_label == pred_label else 'red'\n",
        "\n",
        "        axes[idx].set_title(title, color=color, fontsize=9, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle('Visualization: Predictions tr√™n Test Set (SAM-ViT)',\n",
        "                 fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "print(\"\\nVisualizing predictions...\")\n",
        "visualize_predictions(model, test_loader, device, num_samples=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex5D9teAtJ9F"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# T√çNH IOU V√Ä DICE COEFFICIENT CHO SEGMENTATION MASKS\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    T√≠nh IoU (Intersection over Union) gi·ªØa hai masks\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(mask1, mask2).sum()\n",
        "    union = np.logical_or(mask1, mask2).sum()\n",
        "    if union == 0:\n",
        "        return 1.0 if intersection == 0 else 0.0\n",
        "    return intersection / union\n",
        "\n",
        "def calculate_dice_coefficient(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    T√≠nh Dice coefficient gi·ªØa hai masks\n",
        "    Dice = 2 * |A ‚à© B| / (|A| + |B|)\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(mask1, mask2).sum()\n",
        "    mask1_area = mask1.sum()\n",
        "    mask2_area = mask2.sum()\n",
        "    if mask1_area + mask2_area == 0:\n",
        "        return 1.0 if intersection == 0 else 0.0\n",
        "    return 2.0 * intersection / (mask1_area + mask2_area)\n",
        "\n",
        "def evaluate_segmentation_masks(masks_dir: Path, split: str = 'test', class_ids: List[int] = None):\n",
        "    \"\"\"\n",
        "    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng segmentation masks t·ª´ SAM\n",
        "\n",
        "    Note: Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta ch·ªâ c√≥ masks t·ª´ SAM (kh√¥ng c√≥ ground truth masks ƒë·ªÉ so s√°nh),\n",
        "    n√™n s·∫Ω ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng masks d·ª±a tr√™n:\n",
        "    - S·ªë l∆∞·ª£ng pixels ƒë∆∞·ª£c segment cho m·ªói class\n",
        "    - ƒê·ªô t√°ch bi·ªát gi·ªØa c√°c instances\n",
        "    - Ph√¢n b·ªë m·∫≠t ƒë·ªô\n",
        "    \"\"\"\n",
        "    split_masks_dir = masks_dir / split\n",
        "\n",
        "    if not split_masks_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  Th∆∞ m·ª•c {split_masks_dir} kh√¥ng t·ªìn t·∫°i\")\n",
        "        return None\n",
        "\n",
        "    mask_files = list(split_masks_dir.glob(\"*.png\"))\n",
        "    # B·ªè qua c√°c file _vis.png\n",
        "    mask_files = [f for f in mask_files if not f.name.endswith('_vis.png')]\n",
        "\n",
        "    if class_ids is None:\n",
        "        class_ids = [1, 2, 3]  # Brown, White-backed, Rice leaf miner\n",
        "\n",
        "    results = {\n",
        "        'total_masks': len(mask_files),\n",
        "        'class_stats': {},\n",
        "        'average_mask_quality': {}\n",
        "    }\n",
        "\n",
        "    all_iou_per_class = {cls_id: [] for cls_id in class_ids}\n",
        "    all_dice_per_class = {cls_id: [] for cls_id in class_ids}\n",
        "\n",
        "    print(f\"ƒêang ƒë√°nh gi√° {len(mask_files)} masks t·ª´ split {split}...\")\n",
        "\n",
        "    for mask_file in tqdm_notebook(mask_files, desc='Evaluating masks', leave=False) if USE_TQDM else mask_files:\n",
        "        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            continue\n",
        "\n",
        "        # T√≠nh to√°n cho t·ª´ng class\n",
        "        for class_id in class_ids:\n",
        "            class_mask = (mask == class_id).astype(np.uint8)\n",
        "\n",
        "            if class_mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # T√°ch c√°c instances ri√™ng bi·ªát\n",
        "            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            if len(contours) > 1:\n",
        "                # T√≠nh IoU v√† Dice gi·ªØa c√°c instances (ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t√°ch bi·ªát)\n",
        "                # T·∫°o binary mask cho m·ªói instance v√† so s√°nh v·ªõi to√†n b·ªô class mask\n",
        "                for i, contour1 in enumerate(contours[:5]):  # Gi·ªõi h·∫°n 5 instances ƒë·∫ßu\n",
        "                    instance_mask1 = np.zeros_like(class_mask)\n",
        "                    cv2.fillPoly(instance_mask1, [contour1], 1)\n",
        "\n",
        "                    # So s√°nh instance v·ªõi to√†n b·ªô class mask\n",
        "                    iou = calculate_iou(instance_mask1, class_mask)\n",
        "                    dice = calculate_dice_coefficient(instance_mask1, class_mask)\n",
        "\n",
        "                    all_iou_per_class[class_id].append(iou)\n",
        "                    all_dice_per_class[class_id].append(dice)\n",
        "\n",
        "    # T√≠nh th·ªëng k√™\n",
        "    for class_id in class_ids:\n",
        "        cls_name = CLASS_MAPPING.get(class_id, f\"Class_{class_id}\")\n",
        "        if len(all_iou_per_class[class_id]) > 0:\n",
        "            results['class_stats'][cls_name] = {\n",
        "                'mean_iou': np.mean(all_iou_per_class[class_id]),\n",
        "                'mean_dice': np.mean(all_dice_per_class[class_id]),\n",
        "                'num_instances': len(all_iou_per_class[class_id])\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ƒê√°nh gi√° masks\n",
        "print(\"=\"*70)\n",
        "print(\"ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG SEGMENTATION MASKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "segmentation_results = {}\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"\\nƒêang ƒë√°nh gi√° split: {split}\")\n",
        "    results = evaluate_segmentation_masks(MASKS_DIR, split=split)\n",
        "    if results:\n",
        "        segmentation_results[split] = results\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å SEGMENTATION MASKS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for split, results in segmentation_results.items():\n",
        "    print(f\"\\nSplit: {split.upper()}\")\n",
        "    print(f\"  T·ªïng s·ªë masks: {results['total_masks']}\")\n",
        "    print(f\"  Th·ªëng k√™ theo class:\")\n",
        "    for cls_name, stats in results['class_stats'].items():\n",
        "        print(f\"    {cls_name}:\")\n",
        "        print(f\"      Mean IoU: {stats['mean_iou']:.4f}\")\n",
        "        print(f\"      Mean Dice: {stats['mean_dice']:.4f}\")\n",
        "        print(f\"      S·ªë instances: {stats['num_instances']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pCftkfLtLZN"
      },
      "source": [
        "## 4.2. X√°c ƒë·ªãnh m·∫≠t ƒë·ªô r·∫ßy n√¢u t·ª´ Masks\n",
        "\n",
        "T√≠nh to√°n m·∫≠t ƒë·ªô r·∫ßy n√¢u (s·ªë l∆∞·ª£ng c√° th·ªÉ/ƒë∆°n v·ªã di·ªán t√≠ch) t·ª´ segmentation masks ƒë·ªÉ h·ªó tr·ª£ ƒë√°nh gi√° m·ª©c ƒë·ªô x√¢m nhi·ªÖm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vb-9LUvtL8w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò R·∫¶Y N√ÇU T·ª™ MASKS\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_planthopper_density(masks_dir: Path, images_dir: Path, split: str = 'test'):\n",
        "    \"\"\"\n",
        "    T√≠nh m·∫≠t ƒë·ªô r·∫ßy n√¢u t·ª´ segmentation masks\n",
        "\n",
        "    M·∫≠t ƒë·ªô ƒë∆∞·ª£c t√≠nh b·∫±ng:\n",
        "    - S·ªë l∆∞·ª£ng instances (c√° th·ªÉ) / ·∫£nh\n",
        "    - S·ªë l∆∞·ª£ng pixels / di·ªán t√≠ch ·∫£nh (pixels/cm¬≤ n·∫øu c√≥ th√¥ng tin scale)\n",
        "    \"\"\"\n",
        "    split_masks_dir = masks_dir / split\n",
        "    split_images_dir = images_dir / split\n",
        "\n",
        "    if not split_masks_dir.exists() or not split_images_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i\")\n",
        "        return None\n",
        "\n",
        "    mask_files = list(split_masks_dir.glob(\"*.png\"))\n",
        "    mask_files = [f for f in mask_files if not f.name.endswith('_vis.png')]\n",
        "\n",
        "    density_stats = {\n",
        "        'total_images': 0,\n",
        "        'images_with_planthoppers': 0,\n",
        "        'total_instances': 0,\n",
        "        'instances_per_image': [],\n",
        "        'pixels_per_image': [],\n",
        "        'class_density': {1: [], 2: [], 3: []},  # Brown, White-backed, Rice leaf miner\n",
        "        'per_image_stats': []\n",
        "    }\n",
        "\n",
        "    print(f\"ƒêang t√≠nh m·∫≠t ƒë·ªô r·∫ßy t·ª´ {len(mask_files)} masks...\")\n",
        "\n",
        "    for mask_file in tqdm_notebook(mask_files, desc='Calculating density', leave=False) if USE_TQDM else mask_files:\n",
        "        mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            continue\n",
        "\n",
        "        # ƒê·ªçc ·∫£nh ƒë·ªÉ l·∫•y k√≠ch th∆∞·ªõc\n",
        "        img_file = split_images_dir / f\"{mask_file.stem}.jpg\"\n",
        "        if not img_file.exists():\n",
        "            img_file = split_images_dir / f\"{mask_file.stem}.png\"\n",
        "\n",
        "        img_height, img_width = mask.shape\n",
        "        image_area_pixels = img_height * img_width\n",
        "\n",
        "        # ƒê·∫øm instances cho t·ª´ng class\n",
        "        total_instances = 0\n",
        "        image_stats = {\n",
        "            'image_name': mask_file.stem,\n",
        "            'image_size': (img_width, img_height),\n",
        "            'instances': {},\n",
        "            'pixel_counts': {},\n",
        "            'density': {}\n",
        "        }\n",
        "\n",
        "        for class_id in [1, 2, 3]:\n",
        "            cls_name = CLASS_MAPPING.get(class_id, f\"Class_{class_id}\")\n",
        "            class_mask = (mask == class_id).astype(np.uint8)\n",
        "            pixel_count = class_mask.sum()\n",
        "\n",
        "            if pixel_count == 0:\n",
        "                image_stats['instances'][cls_name] = 0\n",
        "                image_stats['pixel_counts'][cls_name] = 0\n",
        "                image_stats['density'][cls_name] = 0.0\n",
        "                continue\n",
        "\n",
        "            # ƒê·∫øm s·ªë instances (contours)\n",
        "            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            num_instances = len(contours)\n",
        "            total_instances += num_instances\n",
        "\n",
        "            # M·∫≠t ƒë·ªô (instances / di·ªán t√≠ch ·∫£nh * 10000 ƒë·ªÉ chu·∫©n h√≥a)\n",
        "            density = (num_instances / image_area_pixels) * 10000 if image_area_pixels > 0 else 0\n",
        "\n",
        "            image_stats['instances'][cls_name] = num_instances\n",
        "            image_stats['pixel_counts'][cls_name] = pixel_count\n",
        "            image_stats['density'][cls_name] = density\n",
        "\n",
        "            density_stats['class_density'][class_id].append({\n",
        "                'instances': num_instances,\n",
        "                'pixels': pixel_count,\n",
        "                'density': density\n",
        "            })\n",
        "\n",
        "        density_stats['total_images'] += 1\n",
        "        if total_instances > 0:\n",
        "            density_stats['images_with_planthoppers'] += 1\n",
        "            density_stats['total_instances'] += total_instances\n",
        "            density_stats['instances_per_image'].append(total_instances)\n",
        "            density_stats['pixels_per_image'].append(mask[mask > 0].sum())\n",
        "            density_stats['per_image_stats'].append(image_stats)\n",
        "\n",
        "    # T√≠nh th·ªëng k√™ t·ªïng h·ª£p\n",
        "    if len(density_stats['instances_per_image']) > 0:\n",
        "        density_stats['mean_instances_per_image'] = np.mean(density_stats['instances_per_image'])\n",
        "        density_stats['std_instances_per_image'] = np.std(density_stats['instances_per_image'])\n",
        "        density_stats['max_instances_per_image'] = np.max(density_stats['instances_per_image'])\n",
        "        density_stats['min_instances_per_image'] = np.min(density_stats['instances_per_image'])\n",
        "\n",
        "        # Th·ªëng k√™ theo class\n",
        "        for class_id in [1, 2, 3]:\n",
        "            cls_name = CLASS_MAPPING.get(class_id, f\"Class_{class_id}\")\n",
        "            if len(density_stats['class_density'][class_id]) > 0:\n",
        "                instances_list = [d['instances'] for d in density_stats['class_density'][class_id]]\n",
        "                density_stats[f'{cls_name}_mean_instances'] = np.mean(instances_list)\n",
        "                density_stats[f'{cls_name}_mean_density'] = np.mean([d['density'] for d in density_stats['class_density'][class_id]])\n",
        "\n",
        "    return density_stats\n",
        "\n",
        "# T√≠nh m·∫≠t ƒë·ªô r·∫ßy\n",
        "print(\"=\"*70)\n",
        "print(\"X√ÅC ƒê·ªäNH M·∫¨T ƒê·ªò R·∫¶Y N√ÇU T·ª™ MASKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "density_results = {}\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"\\nƒêang t√≠nh m·∫≠t ƒë·ªô cho split: {split}\")\n",
        "    results = calculate_planthopper_density(MASKS_DIR, IMAGES_DIR, split=split)\n",
        "    if results:\n",
        "        density_results[split] = results\n",
        "\n",
        "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"K·∫æT QU·∫¢ M·∫¨T ƒê·ªò R·∫¶Y N√ÇU\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for split, results in density_results.items():\n",
        "    print(f\"\\nSplit: {split.upper()}\")\n",
        "    print(f\"  T·ªïng s·ªë ·∫£nh: {results['total_images']}\")\n",
        "    print(f\"  S·ªë ·∫£nh c√≥ r·∫ßy: {results['images_with_planthoppers']}\")\n",
        "    print(f\"  T·ªïng s·ªë c√° th·ªÉ (instances): {results['total_instances']}\")\n",
        "\n",
        "    if 'mean_instances_per_image' in results:\n",
        "        print(f\"\\n  Th·ªëng k√™ m·∫≠t ƒë·ªô:\")\n",
        "        print(f\"    Trung b√¨nh: {results['mean_instances_per_image']:.2f} c√° th·ªÉ/·∫£nh\")\n",
        "        print(f\"    ƒê·ªô l·ªách chu·∫©n: {results['std_instances_per_image']:.2f}\")\n",
        "        print(f\"    T·ªëi ƒëa: {results['max_instances_per_image']} c√° th·ªÉ/·∫£nh\")\n",
        "        print(f\"    T·ªëi thi·ªÉu: {results['min_instances_per_image']} c√° th·ªÉ/·∫£nh\")\n",
        "\n",
        "        print(f\"\\n  M·∫≠t ƒë·ªô theo lo√†i:\")\n",
        "        for class_id in [1, 2, 3]:\n",
        "            cls_name = CLASS_MAPPING.get(class_id, f\"Class_{class_id}\")\n",
        "            mean_inst_key = f'{cls_name}_mean_instances'\n",
        "            mean_dens_key = f'{cls_name}_mean_density'\n",
        "            if mean_inst_key in results:\n",
        "                print(f\"    {cls_name}:\")\n",
        "                print(f\"      Trung b√¨nh: {results[mean_inst_key]:.2f} c√° th·ªÉ/·∫£nh\")\n",
        "                print(f\"      M·∫≠t ƒë·ªô chu·∫©n h√≥a: {results[mean_dens_key]:.4f}\")\n",
        "\n",
        "# Visualize ph√¢n b·ªë m·∫≠t ƒë·ªô\n",
        "if density_results:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Ph√¢n b·ªë s·ªë instances/·∫£nh\n",
        "    all_instances = []\n",
        "    for split, results in density_results.items():\n",
        "        all_instances.extend(results.get('instances_per_image', []))\n",
        "\n",
        "    if all_instances:\n",
        "        axes[0].hist(all_instances, bins=30, edgecolor='black', alpha=0.7)\n",
        "        axes[0].set_xlabel('S·ªë l∆∞·ª£ng c√° th·ªÉ r·∫ßy/·∫£nh', fontsize=11)\n",
        "        axes[0].set_ylabel('S·ªë l∆∞·ª£ng ·∫£nh', fontsize=11)\n",
        "        axes[0].set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng c√° th·ªÉ r·∫ßy tr√™n m·ªói ·∫£nh', fontsize=12, fontweight='bold')\n",
        "        axes[0].axvline(np.mean(all_instances), color='red', linestyle='--',\n",
        "                       label=f'Trung b√¨nh: {np.mean(all_instances):.2f}')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # So s√°nh m·∫≠t ƒë·ªô theo lo√†i\n",
        "    class_names = ['Brown\\nPlanthopper', 'White-backed\\nPlanthopper', 'Rice Leaf\\nMiner']\n",
        "    mean_instances = []\n",
        "    for class_id in [1, 2, 3]:\n",
        "        cls_name = CLASS_MAPPING[class_id]\n",
        "        all_instances_class = []\n",
        "        for split, results in density_results.items():\n",
        "            for item in results['class_density'][class_id]:\n",
        "                all_instances_class.append(item['instances'])\n",
        "        mean_instances.append(np.mean(all_instances_class) if all_instances_class else 0)\n",
        "\n",
        "    axes[1].bar(class_names, mean_instances, color=['#ff6b6b', '#4ecdc4', '#45b7d1'], alpha=0.7)\n",
        "    axes[1].set_ylabel('S·ªë l∆∞·ª£ng c√° th·ªÉ trung b√¨nh/·∫£nh', fontsize=11)\n",
        "    axes[1].set_title('So s√°nh m·∫≠t ƒë·ªô theo lo√†i r·∫ßy', fontsize=12, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIAV2WcatOUx"
      },
      "source": [
        "## 4.3. So s√°nh v·ªõi m√¥ h√¨nh tham chi·∫øu SwinT-YOLOv8-p2\n",
        "\n",
        "So s√°nh k·∫øt qu·∫£ c·ªßa SAM-ViT v·ªõi m√¥ h√¨nh tham chi·∫øu SwinT-YOLOv8-p2 tr√™n c√°c metrics: Precision, Recall, F1-score, mAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKV4InuftPOW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SO S√ÅNH V·ªöI SWINT-YOLOV8-P2\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SO S√ÅNH V·ªöI M√î H√åNH THAM CHI·∫æU: SWINT-YOLOV8-P2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# K·∫øt qu·∫£ t·ª´ SwinT-YOLOv8-p2 (t·ª´ ƒë·ªÅ t√†i)\n",
        "swint_yolov8_results = {\n",
        "    'mAP@0.5': 0.847,\n",
        "    'F1_score': 0.899,\n",
        "    'Precision': 0.857,  # Gi·∫£ ƒë·ªãnh t·ª´ mAP v√† F1\n",
        "    'Recall': 0.943,     # Gi·∫£ ƒë·ªãnh t·ª´ mAP v√† F1\n",
        "    'FPS': 16.69,\n",
        "    'description': 'SwinT-YOLOv8-p2: Detection model v·ªõi Swin Transformer v√† SCConv'\n",
        "}\n",
        "\n",
        "# K·∫øt qu·∫£ t·ª´ SAM-ViT\n",
        "sam_vit_results = {\n",
        "    'Accuracy': test_results['accuracy'] / 100,  # Chuy·ªÉn t·ª´ % sang decimal\n",
        "    'F1_score': test_results['f1'],\n",
        "    'Precision': test_results['precision'],\n",
        "    'Recall': test_results['recall'],\n",
        "    'description': 'SAM-ViT: Segmentation (SAM) + Classification (ViT)'\n",
        "}\n",
        "\n",
        "# So s√°nh\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['mAP@0.5 / Accuracy', 'Precision', 'Recall', 'F1-Score', 'FPS'],\n",
        "    'SwinT-YOLOv8-p2': [\n",
        "        swint_yolov8_results['mAP@0.5'],\n",
        "        swint_yolov8_results['Precision'],\n",
        "        swint_yolov8_results['Recall'],\n",
        "        swint_yolov8_results['F1_score'],\n",
        "        swint_yolov8_results['FPS']\n",
        "    ],\n",
        "    'SAM-ViT': [\n",
        "        sam_vit_results['Accuracy'],\n",
        "        sam_vit_results['Precision'],\n",
        "        sam_vit_results['Recall'],\n",
        "        sam_vit_results['F1_score'],\n",
        "        'N/A (Classification task)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nB·∫£ng so s√°nh k·∫øt qu·∫£:\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# T√≠nh ph·∫ßn trƒÉm c·∫£i thi·ªán\n",
        "improvement = {}\n",
        "for metric in ['Precision', 'Recall', 'F1_score']:\n",
        "    sam_vit_val = sam_vit_results[metric]\n",
        "    swint_val = swint_yolov8_results[metric]\n",
        "    if isinstance(sam_vit_val, (int, float)) and isinstance(swint_val, (int, float)):\n",
        "        improvement[metric] = ((sam_vit_val - swint_val) / swint_val) * 100\n",
        "\n",
        "print(\"\\nPh·∫ßn trƒÉm c·∫£i thi·ªán c·ªßa SAM-ViT so v·ªõi SwinT-YOLOv8-p2:\")\n",
        "for metric, pct in improvement.items():\n",
        "    sign = \"+\" if pct >= 0 else \"\"\n",
        "    print(f\"  {metric}: {sign}{pct:.2f}%\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart so s√°nh metrics\n",
        "metrics_compare = ['Precision', 'Recall', 'F1-Score']\n",
        "swint_vals = [swint_yolov8_results[m] for m in ['Precision', 'Recall', 'F1_score']]\n",
        "sam_vit_vals = [sam_vit_results[m] for m in ['Precision', 'Recall', 'F1_score']]\n",
        "\n",
        "x = np.arange(len(metrics_compare))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, swint_vals, width, label='SwinT-YOLOv8-p2', alpha=0.8, color='#3498db')\n",
        "axes[0].bar(x + width/2, sam_vit_vals, width, label='SAM-ViT', alpha=0.8, color='#e74c3c')\n",
        "axes[0].set_ylabel('Score', fontsize=11)\n",
        "axes[0].set_title('So s√°nh Precision, Recall, F1-Score', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(metrics_compare)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "axes[0].set_ylim([0, 1])\n",
        "\n",
        "# Accuracy/Detection rate comparison\n",
        "models = ['SwinT-YOLOv8-p2\\n(mAP@0.5)', 'SAM-ViT\\n(Accuracy)']\n",
        "acc_values = [swint_yolov8_results['mAP@0.5'], sam_vit_results['Accuracy']]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "axes[1].bar(models, acc_values, alpha=0.8, color=colors)\n",
        "axes[1].set_ylabel('Score', fontsize=11)\n",
        "axes[1].set_title('So s√°nh mAP@0.5 (Detection) vs Accuracy (Classification)',\n",
        "                  fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].set_ylim([0, 1])\n",
        "\n",
        "# Th√™m gi√° tr·ªã tr√™n bars\n",
        "for i, (model, val) in enumerate(zip(models, acc_values)):\n",
        "    axes[1].text(i, val + 0.02, f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NH·∫¨N X√âT SO S√ÅNH\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "üìä SwinT-YOLOv8-p2:\n",
        "   - M√¥ h√¨nh Detection (YOLO) v·ªõi Swin Transformer\n",
        "   - T·ªëc ƒë·ªô inference cao (16.69 FPS)\n",
        "   - Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng d·ª±a tr√™n bounding boxes\n",
        "   - mAP@0.5 = 0.847, F1 = 0.899\n",
        "\n",
        "üìä SAM-ViT (ƒê·ªÅ xu·∫•t):\n",
        "   - M√¥ h√¨nh k·∫øt h·ª£p Segmentation (SAM) + Classification (ViT)\n",
        "   - SAM t·∫°o masks ch√≠nh x√°c ·ªü c·∫•p ƒë·ªô pixel\n",
        "   - ViT ph√¢n lo·∫°i t·ª´ng ROI ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ masks\n",
        "   - ∆Øu ƒëi·ªÉm: Ph√¢n ƒëo·∫°n chi ti·∫øt, x√°c ƒë·ªãnh m·∫≠t ƒë·ªô ch√≠nh x√°c\n",
        "   - Nh∆∞·ª£c ƒëi·ªÉm: Ch·∫≠m h∆°n (2-stage pipeline)\n",
        "\n",
        "üéØ K·∫øt lu·∫≠n:\n",
        "   - SAM-ViT ph√π h·ª£p cho b√†i to√°n c·∫ßn ph√¢n ƒëo·∫°n chi ti·∫øt v√† x√°c ƒë·ªãnh m·∫≠t ƒë·ªô\n",
        "   - SwinT-YOLOv8-p2 ph√π h·ª£p cho b√†i to√°n real-time detection\n",
        "   - Hai ph∆∞∆°ng ph√°p b·ªï sung cho nhau: SwinT-YOLOv8-p2 ƒë·ªÉ detect nhanh,\n",
        "     SAM-ViT ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt v√† ƒë√°nh gi√° m·∫≠t ƒë·ªô\n",
        "\"\"\")\n",
        "\n",
        "# L∆∞u k·∫øt qu·∫£ so s√°nh\n",
        "comparison_csv = Path(\"/content/drive/MyDrive/ViT_checkpoints/comparison_with_swint_yolov8.csv\")\n",
        "comparison_df.to_csv(comparison_csv, index=False)\n",
        "print(f\"\\n‚úì ƒê√£ l∆∞u b·∫£ng so s√°nh: {comparison_csv}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"T√ìM T·∫ÆT K·∫æT QU·∫¢ SAM-ViT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"  F1 Score: {test_results['f1']:.4f}\")\n",
        "\n",
        "if segmentation_results:\n",
        "    print(f\"\\nSegmentation Quality:\")\n",
        "    for split, seg_results in segmentation_results.items():\n",
        "        print(f\"  {split.upper()}:\")\n",
        "        for cls_name, stats in seg_results.get('class_stats', {}).items():\n",
        "            print(f\"    {cls_name} - Mean IoU: {stats['mean_iou']:.4f}, Mean Dice: {stats['mean_dice']:.4f}\")\n",
        "\n",
        "if density_results:\n",
        "    print(f\"\\nDensity Analysis:\")\n",
        "    for split, dens_results in density_results.items():\n",
        "        if 'mean_instances_per_image' in dens_results:\n",
        "            print(f\"  {split.upper()}: {dens_results['mean_instances_per_image']:.2f} c√° th·ªÉ/·∫£nh\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ƒê·ªÄ XU·∫§T H∆Ø·ªöNG M·ªû R·ªòNG\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "1. T·ªëi ∆∞u hi·ªáu su·∫•t:\n",
        "   - C·∫£i thi·ªán t·ªëc ƒë·ªô inference b·∫±ng c√°ch t·ªëi ∆∞u SAM pipeline\n",
        "   - S·ª≠ d·ª•ng SAM nhanh h∆°n ho·∫∑c quantize model\n",
        "   - K·∫øt h·ª£p SAM-ViT v·ªõi SwinT-YOLOv8-p2: d√πng YOLO ƒë·ªÉ detect nhanh,\n",
        "     SAM-ViT ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt nh·ªØng v√πng c√≥ r·∫ßy\n",
        "\n",
        "2. C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c:\n",
        "   - Fine-tune SAM tr√™n d·ªØ li·ªáu r·∫ßy n√¢u c·ª• th·ªÉ\n",
        "   - Ensemble nhi·ªÅu ViT models v·ªõi c√°c architecture kh√°c nhau\n",
        "   - S·ª≠ d·ª•ng attention mechanisms ƒë·ªÉ t·∫≠p trung v√†o v√πng r·∫ßy nh·ªè\n",
        "\n",
        "3. X·ª≠ l√Ω m·∫≠t ƒë·ªô cao:\n",
        "   - X·ª≠ l√Ω tr∆∞·ªùng h·ª£p r·∫ßy ch·ªìng l·∫•p (overlapping instances)\n",
        "   - C·∫£i thi·ªán kh·∫£ nƒÉng ph√¢n t√°ch c√°c c√° th·ªÉ g·∫ßn nhau\n",
        "   - S·ª≠ d·ª•ng instance segmentation thay v√¨ semantic segmentation\n",
        "\n",
        "4. ·ª®ng d·ª•ng th·ª±c t·∫ø:\n",
        "   - T√≠ch h·ª£p v√†o h·ªá th·ªëng gi√°m s√°t t·ª± ƒë·ªông tr√™n ƒë·ªìng ru·ªông\n",
        "   - Ph√°t tri·ªÉn ·ª©ng d·ª•ng mobile ƒë·ªÉ n√¥ng d√¢n s·ª≠ d·ª•ng\n",
        "   - K·∫øt h·ª£p v·ªõi IoT sensors v√† drone ƒë·ªÉ gi√°m s√°t quy m√¥ l·ªõn\n",
        "   - X√¢y d·ª±ng h·ªá th·ªëng d·ª± b√°o d·ªãch h·∫°i d·ª±a tr√™n m·∫≠t ƒë·ªô\n",
        "\n",
        "5. ƒê√°nh gi√° m·ªü r·ªông:\n",
        "   - Test tr√™n nhi·ªÅu ƒëi·ªÅu ki·ªán √°nh s√°ng, g√≥c ch·ª•p kh√°c nhau\n",
        "   - ƒê√°nh gi√° hi·ªáu qu·∫£ tr√™n c√°c giai ƒëo·∫°n ph√°t tri·ªÉn kh√°c nhau c·ªßa l√∫a\n",
        "   - Cross-validation ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh v√† kh·∫£ nƒÉng t·ªïng qu√°t h√≥a\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n‚úì Ho√†n th√†nh ph√¢n t√≠ch v√† so s√°nh!\")\n",
        "\n",
        "# =============================================================================\n",
        "# L∆ØU MODEL V√Ä K·∫æT QU·∫¢\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"L∆ØU MODEL V√Ä K·∫æT QU·∫¢\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# L∆∞u v√†o Google Drive\n",
        "save_dir = Path(\"/content/drive/MyDrive/ViT_checkpoints\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# L∆∞u model cu·ªëi c√πng\n",
        "final_checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': NUM_EPOCHS,\n",
        "    'best_val_f1': best_val_f1,\n",
        "    'best_epoch': best_epoch,\n",
        "    'test_results': test_results,\n",
        "    'class_names': class_names,\n",
        "    'history': history,\n",
        "    'segmentation_results': segmentation_results,\n",
        "    'density_results': density_results,\n",
        "    'config': {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'class_mapping': CLASS_MAPPING\n",
        "    }\n",
        "}\n",
        "\n",
        "final_path = save_dir / 'vit_classifier_final.pth'\n",
        "torch.save(final_checkpoint, final_path)\n",
        "print(f\"‚úì Final model ƒë√£ ƒë∆∞·ª£c l∆∞u: {final_path}\")\n",
        "\n",
        "# Export predictions v√† results\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': [class_names[i] for i in test_results['labels']],\n",
        "    'predicted_label': [class_names[i] for i in test_results['predictions']],\n",
        "    'correct': test_results['labels'] == test_results['predictions']\n",
        "})\n",
        "\n",
        "results_csv = save_dir / 'test_results.csv'\n",
        "predictions_df.to_csv(results_csv, index=False)\n",
        "print(f\"‚úì Test results ƒë√£ ƒë∆∞·ª£c l∆∞u: {results_csv}\")\n",
        "\n",
        "# L∆∞u summary v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin\n",
        "summary = {\n",
        "    'model': 'SAM-ViT',\n",
        "    'best_val_f1': best_val_f1,\n",
        "    'best_epoch': best_epoch,\n",
        "    'test_metrics': {\n",
        "        'accuracy': test_results['accuracy'],\n",
        "        'precision': test_results['precision'],\n",
        "        'recall': test_results['recall'],\n",
        "        'f1': test_results['f1']\n",
        "    },\n",
        "    'per_class_metrics': test_results['per_class'],\n",
        "    'segmentation_quality': segmentation_results,\n",
        "    'density_analysis': density_results,\n",
        "    'comparison_with_swint_yolov8': {\n",
        "        'swint_yolov8': swint_yolov8_results,\n",
        "        'sam_vit': sam_vit_results,\n",
        "        'improvement': improvement\n",
        "    }\n",
        "}\n",
        "\n",
        "summary_json = save_dir / 'summary.json'\n",
        "with open(summary_json, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "print(f\"‚úì Summary ƒë√£ ƒë∆∞·ª£c l∆∞u: {summary_json}\")\n",
        "\n",
        "# L∆∞u density results\n",
        "if density_results:\n",
        "    density_json = save_dir / 'density_results.json'\n",
        "    with open(density_json, 'w', encoding='utf-8') as f:\n",
        "        json.dump(density_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "    print(f\"‚úì Density results ƒë√£ ƒë∆∞·ª£c l∆∞u: {density_json}\")\n",
        "\n",
        "print(f\"\\nüìÅ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {save_dir}\")\n",
        "# Export predictions summary\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': [class_names[i] for i in test_results['labels']],\n",
        "    'predicted_label': [class_names[i] for i in test_results['predictions']],\n",
        "    'correct': test_results['labels'] == test_results['predictions']\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"T√ìM T·∫ÆT PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(predictions_df.groupby('true_label')['correct'].agg(['count', 'sum', 'mean']))\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
